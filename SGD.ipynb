{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_loss(self, y_pred, y_true):\n",
    "        errors = y_pred - y_true\n",
    "        return np.mean(errors**2)\n",
    "\n",
    "    def get_coeff_gradient(self, y_pred, y_true, X):\n",
    "        errors = y_pred - y_true\n",
    "        return (2 / len(X)) * np.dot(errors, X)\n",
    "\n",
    "    def get_bias_gradient(self, y_pred, y_true, X):\n",
    "        errors = y_pred - y_true\n",
    "        return (2 / len(X)) * np.sum(errors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(\n",
    "        self,\n",
    "        param,\n",
    "        lr=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "    ):\n",
    "        self.param = param\n",
    "        self.beta_m = beta_1\n",
    "        self.beta_v = beta_2\n",
    "        self.lr = lr\n",
    "\n",
    "        # other parameters\n",
    "        self.momentum = 0\n",
    "        self.prev_momentum = 0\n",
    "        self.v = 0\n",
    "        self.prev_v = 0\n",
    "\n",
    "    def step(self, gradient, epoch):\n",
    "        self.prev_momentum = self.momentum\n",
    "        self.prev_v = self.v\n",
    "\n",
    "        # calculate the moving average of the momentum and the squared gradients\n",
    "        self.momentum = (self.beta_m * self.prev_momentum) + (\n",
    "            1 - self.beta_m\n",
    "        ) * gradient\n",
    "        self.v = (self.beta_v * self.prev_v) + (1 - self.beta_v) * (gradient**2)\n",
    "\n",
    "        # bias correction\n",
    "        momentum_hat = self.momentum / (1 - self.beta_m ** (epoch + 1))\n",
    "        v_hat = self.v / (1 - self.beta_v ** (epoch + 1))\n",
    "\n",
    "        learning_rate = self.lr / (np.sqrt(v_hat) + 1e-8)\n",
    "        self.param -= learning_rate * momentum_hat\n",
    "\n",
    "    def get_param(self):\n",
    "        return self.param\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "        # add bool to check if the layer has been initialized\n",
    "        self.initialized = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        if not self.initialized:\n",
    "            raise Exception(\"Layer not initialized\")\n",
    "        return np.dot(self.weights, X.T) + self.bias\n",
    "\n",
    "    def initialize(self, input_dim, output_dim=1):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = 1\n",
    "\n",
    "        self.weights = np.random.randn(self.input_dim)\n",
    "        self.bias = np.random.randn(1)\n",
    "        if self.verbose:\n",
    "            print(\"Weights: \", self.weights, \"Shape: \", self.weights.shape)\n",
    "            print(\"Bias: \", self.bias, \"Shape: \", self.bias.shape)\n",
    "        \n",
    "        self.initialized = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layer, loss, otimizer):\n",
    "        self.layer = layer(verbose=False)\n",
    "        self.loss = loss()\n",
    "        self.optimizer = otimizer\n",
    "        self.initialized = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        if not self.initialized:\n",
    "            raise Exception(\"Model not initialized. Please call the fit method\")\n",
    "        return self.layer.forward(X)\n",
    "\n",
    "    def backward(self, y_pred, y_true, X):\n",
    "        if not self.initialized:\n",
    "            raise Exception(\"Model not initialized. Please call the fit method\")\n",
    "        # Get the gradient\n",
    "        coeff_gradient = self.loss.get_coeff_gradient(y_pred, y_true, X)\n",
    "        bias_gradient = self.loss.get_bias_gradient(y_pred, y_true, X)\n",
    "        return coeff_gradient, bias_gradient\n",
    "\n",
    "    def fit(self, X, y, epochs=100):\n",
    "        # Initialize the layer\n",
    "        input_dim = X.shape[-1]\n",
    "        self.layer.initialize(input_dim)\n",
    "\n",
    "        # initialize the optimizers\n",
    "        self.optimizers = [\n",
    "            self.optimizer(param) for param in [*self.layer.weights, self.layer.bias]\n",
    "        ]\n",
    "        self.initialized = True\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for idx, x_i in enumerate(X):\n",
    "\n",
    "                self.layer.weights = [\n",
    "                    optimizer.get_param() for optimizer in self.optimizers[:-1]\n",
    "                ]\n",
    "                self.layer.bias = self.optimizers[-1].get_param()\n",
    "\n",
    "                y_i = y[idx]\n",
    "                # Forward pass\n",
    "                # [0] IS BECAUSE of bias = [b] the float turns into [float] so we extract float by [float][0]\n",
    "                y_i_pred = self.forward(x_i)[0]\n",
    "\n",
    "                loss = self.loss.get_loss(y_i_pred, y[idx])\n",
    "                print(f\"\\nEpoch: {epoch}.{idx} | Loss: {loss}\")\n",
    "\n",
    "                # Backward pass\n",
    "                coeff_gradients, bias_gradient = self.backward(y_i_pred, y_i, x_i)\n",
    "\n",
    "                # Update the weights\n",
    "                for optimizer in self.optimizers[:-1]:\n",
    "                    optimizer.step(coeff_gradients, epoch)\n",
    "\n",
    "                self.optimizers[-1].step(bias_gradient, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0.0 | Loss: 25.122801957282302\n",
      "\n",
      "Epoch: 0.1 | Loss: 64.69233191048336\n",
      "\n",
      "Epoch: 0.2 | Loss: 101.48389780516958\n",
      "\n",
      "Epoch: 0.3 | Loss: 144.85553436419826\n",
      "\n",
      "Epoch: 0.4 | Loss: 193.8395391758756\n",
      "\n",
      "Epoch: 1.0 | Loss: 32.1291277087665\n",
      "\n",
      "Epoch: 1.1 | Loss: 57.871981181077594\n",
      "\n",
      "Epoch: 1.2 | Loss: 90.22501436115556\n",
      "\n",
      "Epoch: 1.3 | Loss: 128.5280016120291\n",
      "\n",
      "Epoch: 1.4 | Loss: 172.02443560247963\n",
      "\n",
      "Epoch: 2.0 | Loss: 29.20247219715777\n",
      "\n",
      "Epoch: 2.1 | Loss: 52.13107982056416\n",
      "\n",
      "Epoch: 2.2 | Loss: 80.89901649320863\n",
      "\n",
      "Epoch: 2.3 | Loss: 115.01683325057843\n",
      "\n",
      "Epoch: 2.4 | Loss: 153.91788200155085\n",
      "\n",
      "Epoch: 3.0 | Loss: 26.734683176024312\n",
      "\n",
      "Epoch: 3.1 | Loss: 47.360217031219356\n",
      "\n",
      "Epoch: 3.2 | Loss: 73.2143788116336\n",
      "\n",
      "Epoch: 3.3 | Loss: 103.91084396712614\n",
      "\n",
      "Epoch: 3.4 | Loss: 139.00048140855125\n",
      "\n",
      "Epoch: 4.0 | Loss: 24.6706061577018\n",
      "\n",
      "Epoch: 4.1 | Loss: 43.39076143940077\n",
      "\n",
      "Epoch: 4.2 | Loss: 66.83463888879184\n",
      "\n",
      "Epoch: 4.3 | Loss: 94.68677738264314\n",
      "\n",
      "Epoch: 4.4 | Loss: 126.57830702517076\n",
      "\n",
      "Epoch: 5.0 | Loss: 22.927141253028186\n",
      "\n",
      "Epoch: 5.1 | Loss: 40.046843749683624\n",
      "\n",
      "Epoch: 5.2 | Loss: 61.46263439118171\n",
      "\n",
      "Epoch: 5.3 | Loss: 86.91047611396192\n",
      "\n",
      "Epoch: 5.4 | Loss: 116.07952437961919\n",
      "\n",
      "Epoch: 6.0 | Loss: 21.43377461646039\n",
      "\n",
      "Epoch: 6.1 | Loss: 37.187643197255966\n",
      "\n",
      "Epoch: 6.2 | Loss: 56.868845196700576\n",
      "\n",
      "Epoch: 6.3 | Loss: 80.25195244296545\n",
      "\n",
      "Epoch: 6.4 | Loss: 107.06970264054493\n",
      "\n",
      "Epoch: 7.0 | Loss: 20.13597141950143\n",
      "\n",
      "Epoch: 7.1 | Loss: 34.7064684764822\n",
      "\n",
      "Epoch: 7.2 | Loss: 52.881615751887345\n",
      "\n",
      "Epoch: 7.3 | Loss: 74.46574196568666\n",
      "\n",
      "Epoch: 7.4 | Loss: 99.22529038214131\n",
      "\n",
      "Epoch: 8.0 | Loss: 18.992555643983412\n",
      "\n",
      "Epoch: 8.1 | Loss: 32.52353715102406\n",
      "\n",
      "Epoch: 8.2 | Loss: 49.37330127586901\n",
      "\n",
      "Epoch: 8.3 | Loss: 69.36963026378997\n",
      "\n",
      "Epoch: 8.4 | Loss: 92.30567570494094\n",
      "\n",
      "Epoch: 9.0 | Loss: 17.972554981609544\n",
      "\n",
      "Epoch: 9.1 | Loss: 30.579144430854537\n",
      "\n",
      "Epoch: 9.2 | Loss: 46.24861593951289\n",
      "\n",
      "Epoch: 9.3 | Loss: 64.82761148067453\n",
      "\n",
      "Epoch: 9.4 | Loss: 86.13089818128921\n",
      "\n",
      "Epoch: 10.0 | Loss: 17.052575638341516\n",
      "\n",
      "Epoch: 10.1 | Loss: 28.828320992330973\n",
      "\n",
      "Epoch: 10.2 | Loss: 43.43585416578039\n",
      "\n",
      "Epoch: 10.3 | Loss: 60.73726312679849\n",
      "\n",
      "Epoch: 10.4 | Loss: 80.5651407923813\n",
      "\n",
      "Epoch: 11.0 | Loss: 16.214810218810996\n",
      "\n",
      "Epoch: 11.1 | Loss: 27.236884948697078\n",
      "\n",
      "Epoch: 11.2 | Loss: 40.88050858924566\n",
      "\n",
      "Epoch: 11.3 | Loss: 57.02063397083012\n",
      "\n",
      "Epoch: 11.4 | Loss: 75.50480516224155\n",
      "\n",
      "Epoch: 12.0 | Loss: 15.445573384291006\n",
      "\n",
      "Epoch: 12.1 | Loss: 25.77858026052512\n",
      "\n",
      "Epoch: 12.2 | Loss: 38.54068335451966\n",
      "\n",
      "Epoch: 12.3 | Loss: 53.61771746811438\n",
      "\n",
      "Epoch: 12.4 | Loss: 70.86995970966022\n",
      "\n",
      "Epoch: 13.0 | Loss: 14.734236984078231\n",
      "\n",
      "Epoch: 13.1 | Loss: 24.433013249341926\n",
      "\n",
      "Epoch: 13.2 | Loss: 36.38380237079908\n",
      "\n",
      "Epoch: 13.3 | Loss: 50.48177501531592\n",
      "\n",
      "Epoch: 13.4 | Loss: 66.5982029156868\n",
      "\n",
      "Epoch: 14.0 | Loss: 14.072456911151736\n",
      "\n",
      "Epoch: 14.1 | Loss: 23.184162509873943\n",
      "\n",
      "Epoch: 14.2 | Loss: 34.38423879504979\n",
      "\n",
      "Epoch: 14.3 | Loss: 47.57596976454683\n",
      "\n",
      "Epoch: 14.4 | Loss: 62.64023956138329\n",
      "\n",
      "Epoch: 15.0 | Loss: 13.453610073414433\n",
      "\n",
      "Epoch: 15.1 | Loss: 22.019297692782494\n",
      "\n",
      "Epoch: 15.2 | Loss: 32.52159760907779\n",
      "\n",
      "Epoch: 15.3 | Loss: 44.870927569525584\n",
      "\n",
      "Epoch: 15.4 | Loss: 58.956670289393706\n",
      "\n",
      "Epoch: 16.0 | Loss: 12.872382097208463\n",
      "\n",
      "Epoch: 16.1 | Loss: 20.928189663859175\n",
      "\n",
      "Epoch: 16.2 | Loss: 30.77946195791187\n",
      "\n",
      "Epoch: 16.3 | Loss: 42.34295562169107\n",
      "\n",
      "Epoch: 16.4 | Loss: 55.51564301182216\n",
      "\n",
      "Epoch: 17.0 | Loss: 12.3244632701959\n",
      "\n",
      "Epoch: 17.1 | Loss: 19.90252894438226\n",
      "\n",
      "Epoch: 17.2 | Loss: 29.144470262777435\n",
      "\n",
      "Epoch: 17.3 | Loss: 39.97273002931911\n",
      "\n",
      "Epoch: 17.4 | Loss: 52.291119674979164\n",
      "\n",
      "Epoch: 18.0 | Loss: 11.806322505849975\n",
      "\n",
      "Epoch: 18.1 | Loss: 18.935493805973362\n",
      "\n",
      "Epoch: 18.2 | Loss: 27.605630693056074\n",
      "\n",
      "Epoch: 18.3 | Loss: 37.74431995308173\n",
      "\n",
      "Epoch: 18.4 | Loss: 49.261585293955555\n",
      "\n",
      "Epoch: 19.0 | Loss: 11.315037858946813\n",
      "\n",
      "Epoch: 19.1 | Loss: 18.021426595241905\n",
      "\n",
      "Epoch: 19.2 | Loss: 26.153807181322115\n",
      "\n",
      "Epoch: 19.3 | Loss: 35.64445509452105\n",
      "\n",
      "Epoch: 19.4 | Loss: 46.409077254464975\n",
      "\n",
      "Epoch: 20.0 | Loss: 10.848168302877731\n",
      "\n",
      "Epoch: 20.1 | Loss: 17.155588908491037\n",
      "\n",
      "Epoch: 20.2 | Loss: 24.781330390736432\n",
      "\n",
      "Epoch: 20.3 | Loss: 33.6619705749701\n",
      "\n",
      "Epoch: 20.4 | Loss: 43.71844843184497\n",
      "\n",
      "Epoch: 21.0 | Loss: 10.403655837044306\n",
      "\n",
      "Epoch: 21.1 | Loss: 16.333974671873595\n",
      "\n",
      "Epoch: 21.2 | Loss: 23.481700459849808\n",
      "\n",
      "Epoch: 21.3 | Loss: 31.787382230393018\n",
      "\n",
      "Epoch: 21.4 | Loss: 41.176802484388055\n",
      "\n",
      "Epoch: 22.0 | Loss: 9.97975006673981\n",
      "\n",
      "Epoch: 22.1 | Loss: 15.553166104429172\n",
      "\n",
      "Epoch: 22.2 | Loss: 22.24935774566047\n",
      "\n",
      "Epoch: 22.3 | Loss: 30.012558638049875\n",
      "\n",
      "Epoch: 22.4 | Loss: 38.77305706028135\n",
      "\n",
      "Epoch: 23.0 | Loss: 9.574949572802678\n",
      "\n",
      "Epoch: 23.1 | Loss: 14.810221715842683\n",
      "\n",
      "Epoch: 23.2 | Loss: 21.07950439771757\n",
      "\n",
      "Epoch: 23.3 | Loss: 28.3304655416598\n",
      "\n",
      "Epoch: 23.4 | Loss: 36.49760290032953\n",
      "\n",
      "Epoch: 24.0 | Loss: 9.187955933349297\n",
      "\n",
      "Epoch: 24.1 | Loss: 14.10258844804968\n",
      "\n",
      "Epoch: 24.2 | Loss: 19.96796427500808\n",
      "\n",
      "Epoch: 24.3 | Loss: 26.734964959429714\n",
      "\n",
      "Epoch: 24.4 | Loss: 34.342035492907776\n",
      "\n",
      "Epoch: 25.0 | Loss: 8.817637362991594\n",
      "\n",
      "Epoch: 25.1 | Loss: 13.428032176848793\n",
      "\n",
      "Epoch: 25.2 | Loss: 18.91107204885704\n",
      "\n",
      "Epoch: 25.3 | Loss: 25.220655972781415\n",
      "\n",
      "Epoch: 25.4 | Loss: 32.29894212309878\n",
      "\n",
      "Epoch: 26.0 | Loss: 8.46299972701486\n",
      "\n",
      "Epoch: 26.1 | Loss: 12.78458230031654\n",
      "\n",
      "Epoch: 26.2 | Loss: 17.905584722836736\n",
      "\n",
      "Epoch: 26.3 | Loss: 23.78274757363527\n",
      "\n",
      "Epoch: 26.4 | Loss: 30.361731599000198\n",
      "\n",
      "Epoch: 27.0 | Loss: 8.123163260261292\n",
      "\n",
      "Epoch: 27.1 | Loss: 12.17048723108449\n",
      "\n",
      "Epoch: 27.2 | Loss: 16.948610523956383\n",
      "\n",
      "Epoch: 27.3 | Loss: 22.41695638917757\n",
      "\n",
      "Epoch: 27.4 | Loss: 28.524497149586715\n",
      "\n",
      "Epoch: 28.0 | Loss: 7.797343736661644\n",
      "\n",
      "Epoch: 28.1 | Loss: 11.584178401911544\n",
      "\n",
      "Epoch: 28.2 | Loss: 16.037551372101962\n",
      "\n",
      "Epoch: 28.3 | Loss: 21.119423878975876\n",
      "\n",
      "Epoch: 28.4 | Loss: 26.781905327948504\n",
      "\n",
      "Epoch: 29.0 | Loss: 7.484837140140071\n",
      "\n",
      "Epoch: 29.1 | Loss: 11.024240974024572\n",
      "\n",
      "Epoch: 29.2 | Loss: 15.170056052092013\n",
      "\n",
      "Epoch: 29.3 | Loss: 19.886648901108167\n",
      "\n",
      "Epoch: 29.4 | Loss: 25.129105470913064\n",
      "\n",
      "Epoch: 30.0 | Loss: 7.185007112407749\n",
      "\n",
      "Epoch: 30.1 | Loss: 10.48938986546392\n",
      "\n",
      "Epoch: 30.2 | Loss: 14.343981889666654\n",
      "\n",
      "Epoch: 30.3 | Loss: 18.715432505568252\n",
      "\n",
      "Epoch: 30.4 | Loss: 23.561655536247663\n",
      "\n",
      "Epoch: 31.0 | Loss: 6.897274620170244\n",
      "\n",
      "Epoch: 31.1 | Loss: 9.978450034521067\n",
      "\n",
      "Epoch: 31.2 | Loss: 13.55736323610335\n",
      "\n",
      "Epoch: 31.3 | Loss: 17.602832529043795\n",
      "\n",
      "Epoch: 31.4 | Loss: 22.07546108554503\n",
      "\n",
      "Epoch: 32.0 | Loss: 6.621109409279296\n",
      "\n",
      "Epoch: 32.1 | Loss: 9.49034019134525\n",
      "\n",
      "Epoch: 32.2 | Loss: 12.808385443374457\n",
      "\n",
      "Epoch: 32.3 | Loss: 16.546126102228087\n",
      "\n",
      "Epoch: 32.4 | Loss: 20.666724892349627\n",
      "\n",
      "Epoch: 33.0 | Loss: 6.356022907630929\n",
      "\n",
      "Epoch: 33.1 | Loss: 9.024059290361974\n",
      "\n",
      "Epoch: 33.2 | Loss: 12.09536329664985\n",
      "\n",
      "Epoch: 33.3 | Loss: 15.542778586933172\n",
      "\n",
      "Epoch: 33.4 | Loss: 19.331905193843475\n",
      "\n",
      "Epoch: 34.0 | Loss: 6.101562310242801\n",
      "\n",
      "Epoch: 34.1 | Loss: 8.578675292677387\n",
      "\n",
      "Epoch: 34.2 | Loss: 11.416723087781277\n",
      "\n",
      "Epoch: 34.3 | Loss: 14.5904177697604\n",
      "\n",
      "Epoch: 34.4 | Loss: 18.06768101559856\n",
      "\n",
      "Epoch: 35.0 | Loss: 5.8573056347829375\n",
      "\n",
      "Epoch: 35.1 | Loss: 8.15331579224856\n",
      "\n",
      "Epoch: 35.2 | Loss: 10.770987679719727\n",
      "\n",
      "Epoch: 35.3 | Loss: 13.686812376775514\n",
      "\n",
      "Epoch: 35.4 | Loss: 16.8709233151669\n",
      "\n",
      "Epoch: 36.0 | Loss: 5.622857578121243\n",
      "\n",
      "Epoch: 36.1 | Loss: 7.747160180357503\n",
      "\n",
      "Epoch: 36.2 | Loss: 10.15676404036242\n",
      "\n",
      "Epoch: 36.3 | Loss: 12.829854157592937\n",
      "\n",
      "Epoch: 36.4 | Loss: 15.738670935411553\n",
      "\n",
      "Epoch: 37.0 | Loss: 5.397846037344628\n",
      "\n",
      "Epoch: 37.1 | Loss: 7.359433085742832\n",
      "\n",
      "Epoch: 37.2 | Loss: 9.572732824432602\n",
      "\n",
      "Epoch: 37.3 | Loss: 12.017542930725615\n",
      "\n",
      "Epoch: 37.4 | Loss: 14.668110549917948\n",
      "\n",
      "Epoch: 38.0 | Loss: 5.18191918440605\n",
      "\n",
      "Epoch: 38.1 | Loss: 6.989398876966499\n",
      "\n",
      "Epoch: 38.2 | Loss: 9.017639660537398\n",
      "\n",
      "Epoch: 38.3 | Loss: 11.24797409475946\n",
      "\n",
      "Epoch: 38.4 | Loss: 13.656559933449408\n",
      "\n",
      "Epoch: 39.0 | Loss: 4.974743003863704\n",
      "\n",
      "Epoch: 39.1 | Loss: 6.63635705244661\n",
      "\n",
      "Epoch: 39.2 | Loss: 8.490287862623173\n",
      "\n",
      "Epoch: 39.3 | Loss: 10.519328199105518\n",
      "\n",
      "Epoch: 39.4 | Loss: 12.701454009790956\n",
      "\n",
      "Epoch: 40.0 | Loss: 4.775999219274235\n",
      "\n",
      "Epoch: 40.1 | Loss: 6.29963837447602\n",
      "\n",
      "Epoch: 40.2 | Loss: 7.989532334461188\n",
      "\n",
      "Epoch: 40.3 | Loss: 9.829862239182326\n",
      "\n",
      "Epoch: 40.4 | Loss: 11.800333224624165\n",
      "\n",
      "Epoch: 41.0 | Loss: 4.585383546682568\n",
      "\n",
      "Epoch: 41.1 | Loss: 5.9786016282748875\n",
      "\n",
      "Epoch: 41.2 | Loss: 7.514274475407439\n",
      "\n",
      "Epoch: 41.3 | Loss: 9.177902397953716\n",
      "\n",
      "Epoch: 41.4 | Loss: 10.950833867685828\n",
      "\n",
      "Epoch: 42.0 | Loss: 4.402604224018055\n",
      "\n",
      "Epoch: 42.1 | Loss: 5.672630907058026\n",
      "\n",
      "Epoch: 42.2 | Loss: 7.063457927652653\n",
      "\n",
      "Epoch: 42.3 | Loss: 8.561838001878455\n",
      "\n",
      "Epoch: 42.4 | Loss: 10.150680030474662\n",
      "\n",
      "Epoch: 43.0 | Loss: 4.22738077360732\n",
      "\n",
      "Epoch: 43.1 | Loss: 5.381133340272715\n",
      "\n",
      "Epoch: 43.2 | Loss: 6.636065031159251\n",
      "\n",
      "Epoch: 43.3 | Loss: 7.980116496865518\n",
      "\n",
      "Epoch: 43.4 | Loss: 9.397676936300243\n",
      "\n",
      "Epoch: 44.0 | Loss: 4.0594429618652645\n",
      "\n",
      "Epoch: 44.1 | Loss: 5.103537195373725\n",
      "\n",
      "Epoch: 44.2 | Loss: 6.23111387373137\n",
      "\n",
      "Epoch: 44.3 | Loss: 7.4312392805732115\n",
      "\n",
      "Epoch: 44.4 | Loss: 8.689705420915145\n",
      "\n",
      "Epoch: 45.0 | Loss: 3.8985299258481847\n",
      "\n",
      "Epoch: 45.1 | Loss: 4.839290294360834\n",
      "\n",
      "Epoch: 45.2 | Loss: 5.847655841155267\n",
      "\n",
      "Epoch: 45.3 | Loss: 6.913758252732688\n",
      "\n",
      "Epoch: 45.4 | Loss: 8.024717376181385\n",
      "\n",
      "Epoch: 46.0 | Loss: 3.7443894409953553\n",
      "\n",
      "Epoch: 46.1 | Loss: 4.5878586952846545\n",
      "\n",
      "Epoch: 46.2 | Loss: 5.484773586834331\n",
      "\n",
      "Epoch: 46.3 | Loss: 6.4262729661953895\n",
      "\n",
      "Epoch: 46.4 | Loss: 7.400731997636686\n",
      "\n",
      "Epoch: 47.0 | Loss: 3.596777308240988\n",
      "\n",
      "Epoch: 47.1 | Loss: 4.348725596397987\n",
      "\n",
      "Epoch: 47.2 | Loss: 5.141579352413159\n",
      "\n",
      "Epoch: 47.3 | Loss: 5.9674282789405835\n",
      "\n",
      "Epoch: 47.4 | Loss: 6.815832700566398\n",
      "\n",
      "Epoch: 48.0 | Loss: 3.4554568418990326\n",
      "\n",
      "Epoch: 48.1 | Loss: 4.121390426881349\n",
      "\n",
      "Epoch: 48.2 | Loss: 4.8172135810005\n",
      "\n",
      "Epoch: 48.3 | Loss: 5.53591242199712\n",
      "\n",
      "Epoch: 48.4 | Loss: 6.26816458913769\n",
      "\n",
      "Epoch: 49.0 | Loss: 3.320198442428007\n",
      "\n",
      "Epoch: 49.1 | Loss: 3.905368093331945\n",
      "\n",
      "Epoch: 49.2 | Loss: 4.510843773125322\n",
      "\n",
      "Epoch: 49.3 | Loss: 5.130455410653465\n",
      "\n",
      "Epoch: 49.4 | Loss: 5.755932380011325\n",
      "\n",
      "Epoch: 50.0 | Loss: 3.1907792404664503\n",
      "\n",
      "Epoch: 50.1 | Loss: 3.700188355654487\n",
      "\n",
      "Epoch: 50.2 | Loss: 4.221663542781558\n",
      "\n",
      "Epoch: 50.3 | Loss: 4.749827736866243\n",
      "\n",
      "Epoch: 50.4 | Loss: 5.277398696168221\n",
      "\n",
      "Epoch: 51.0 | Loss: 3.0669828004670556\n",
      "\n",
      "Epoch: 51.1 | Loss: 3.5053953097752486\n",
      "\n",
      "Epoch: 51.2 | Loss: 3.9488918370669794\n",
      "\n",
      "Epoch: 51.3 | Loss: 4.392839289762923\n",
      "\n",
      "Epoch: 51.4 | Loss: 4.830882658915794\n",
      "\n",
      "Epoch: 52.0 | Loss: 2.9485988739092965\n",
      "\n",
      "Epoch: 52.1 | Loss: 3.3205469578320295\n",
      "\n",
      "Epoch: 52.2 | Loss: 3.6917722881820962\n",
      "\n",
      "Epoch: 52.3 | Loss: 4.058338458830829\n",
      "\n",
      "Epoch: 52.4 | Loss: 4.414758716528081\n",
      "\n",
      "Epoch: 53.0 | Loss: 2.835423193484443\n",
      "\n",
      "Epoch: 53.1 | Loss: 3.1452148492654057\n",
      "\n",
      "Epoch: 53.2 | Loss: 3.4495726710770973\n",
      "\n",
      "Epoch: 53.3 | Loss: 3.7452113810090073\n",
      "\n",
      "Epoch: 53.4 | Loss: 4.027455657010969\n",
      "\n",
      "Epoch: 54.0 | Loss: 2.7272573008631555\n",
      "\n",
      "Epoch: 54.1 | Loss: 2.978983778625568\n",
      "\n",
      "Epoch: 54.2 | Loss: 3.2215844439360497\n",
      "\n",
      "Epoch: 54.3 | Loss: 3.4523812986225773\n",
      "\n",
      "Epoch: 54.4 | Loss: 3.667455760301301\n",
      "\n",
      "Epoch: 55.0 | Loss: 2.6239084017053527\n",
      "\n",
      "Epoch: 55.1 | Loss: 2.8214515279754306\n",
      "\n",
      "Epoch: 55.2 | Loss: 3.007122352067429\n",
      "\n",
      "Epoch: 55.3 | Loss: 3.178808000062515\n",
      "\n",
      "Epoch: 55.4 | Loss: 3.333294051995259\n",
      "\n",
      "Epoch: 56.0 | Loss: 2.525189242481128\n",
      "\n",
      "Epoch: 56.1 | Loss: 2.672228643563589\n",
      "\n",
      "Epoch: 56.2 | Loss: 2.8055240787068496\n",
      "\n",
      "Epoch: 56.3 | Loss: 2.9234873194319535\n",
      "\n",
      "Epoch: 56.4 | Loss: 3.023557626611374\n",
      "\n",
      "Epoch: 57.0 | Loss: 2.430918004461293\n",
      "\n",
      "Epoch: 57.1 | Loss: 2.5309382380015264\n",
      "\n",
      "Epoch: 57.2 | Loss: 2.616149928797495\n",
      "\n",
      "Epoch: 57.3 | Loss: 2.6854506751472775\n",
      "\n",
      "Epoch: 57.4 | Loss: 2.7368850135541525\n",
      "\n",
      "Epoch: 58.0 | Loss: 2.340918210924071\n",
      "\n",
      "Epoch: 58.1 | Loss: 2.397215810541306\n",
      "\n",
      "Epoch: 58.2 | Loss: 2.4383825340498024\n",
      "\n",
      "Epoch: 58.3 | Loss: 2.4637646307758434\n",
      "\n",
      "Epoch: 58.4 | Loss: 2.471965563459183\n",
      "\n",
      "Epoch: 59.0 | Loss: 2.2550186442250104\n",
      "\n",
      "Epoch: 59.1 | Loss: 2.2707090792404956\n",
      "\n",
      "Epoch: 59.2 | Loss: 2.271626569538683\n",
      "\n",
      "Epoch: 59.3 | Loss: 2.2575304642768956\n",
      "\n",
      "Epoch: 59.4 | Loss: 2.2275388365569015\n",
      "\n",
      "Epoch: 60.0 | Loss: 2.1730532699023897\n",
      "\n",
      "Epoch: 60.1 | Loss: 2.151077819843661\n",
      "\n",
      "Epoch: 60.2 | Loss: 2.1153084738111168\n",
      "\n",
      "Epoch: 60.3 | Loss: 2.0658837343407215\n",
      "\n",
      "Epoch: 60.4 | Loss: 2.002393978161754\n",
      "\n",
      "Epoch: 61.0 | Loss: 2.0948611654502534\n",
      "\n",
      "Epoch: 61.1 | Loss: 2.0379937071231167\n",
      "\n",
      "Epoch: 61.2 | Loss: 1.9688761659802312\n",
      "\n",
      "Epoch: 61.3 | Loss: 1.8879938347388254\n",
      "\n",
      "Epoch: 61.4 | Loss: 1.795369069436327\n",
      "\n",
      "Epoch: 62.0 | Loss: 2.0202864517939285\n",
      "\n",
      "Epoch: 62.1 | Loss: 1.931140155222338\n",
      "\n",
      "Epoch: 62.2 | Loss: 1.8317987545993908\n",
      "\n",
      "Epoch: 62.3 | Loss: 1.7230635295413719\n",
      "\n",
      "Epoch: 62.4 | Loss: 1.605350444246981\n",
      "\n",
      "Epoch: 63.0 | Loss: 1.9491782258559485\n",
      "\n",
      "Epoch: 63.1 | Loss: 1.8302121542466077\n",
      "\n",
      "Epoch: 63.2 | Loss: 1.7035662342638216\n",
      "\n",
      "Epoch: 63.3 | Loss: 1.5703284637600927\n",
      "\n",
      "Epoch: 63.4 | Loss: 1.4312719652614774\n",
      "\n",
      "Epoch: 64.0 | Loss: 1.8813904929092427\n",
      "\n",
      "Epoch: 64.1 | Loss: 1.7349161009592873\n",
      "\n",
      "Epoch: 64.2 | Loss: 1.583689166896085\n",
      "\n",
      "Epoch: 64.3 | Loss: 1.429056645461392\n",
      "\n",
      "Epoch: 64.4 | Loss: 1.2721142544760458\n",
      "\n",
      "Epoch: 65.0 | Loss: 1.8167820976846396\n",
      "\n",
      "Epoch: 65.1 | Loss: 1.6449696219773469\n",
      "\n",
      "Epoch: 65.2 | Loss: 1.4716983455510904\n",
      "\n",
      "Epoch: 65.3 | Loss: 1.2985478966887214\n",
      "\n",
      "Epoch: 65.4 | Loss: 1.1269038751317093\n",
      "\n",
      "Epoch: 66.0 | Loss: 1.7552166534355522\n",
      "\n",
      "Epoch: 66.1 | Loss: 1.560101388326293\n",
      "\n",
      "Epoch: 66.2 | Loss: 1.3671444393403172\n",
      "\n",
      "Epoch: 66.3 | Loss: 1.1781332716560386\n",
      "\n",
      "Epoch: 66.4 | Loss: 0.9947124635129838\n",
      "\n",
      "Epoch: 67.0 | Loss: 1.696562468367738\n",
      "\n",
      "Epoch: 67.1 | Loss: 1.4800509206183052\n",
      "\n",
      "Epoch: 67.2 | Loss: 1.269597618734924\n",
      "\n",
      "Epoch: 67.3 | Loss: 1.0671744416414248\n",
      "\n",
      "Epoch: 67.4 | Loss: 0.874655810441465\n",
      "\n",
      "Epoch: 68.0 | Loss: 1.6406924690197455\n",
      "\n",
      "Epoch: 68.1 | Loss: 1.4045683844657495\n",
      "\n",
      "Epoch: 68.2 | Loss: 1.1786471610742197\n",
      "\n",
      "Epoch: 68.3 | Loss: 0.9650630468382092\n",
      "\n",
      "Epoch: 68.4 | Loss: 0.7658928934021202\n",
      "\n",
      "Epoch: 69.0 | Loss: 1.5874841203327918\n",
      "\n",
      "Epoch: 69.1 | Loss: 1.3334143760400554\n",
      "\n",
      "Epoch: 69.2 | Loss: 1.0939010365885153\n",
      "\n",
      "Epoch: 69.3 | Loss: 0.8712200161225959\n",
      "\n",
      "Epoch: 69.4 | Loss: 0.6676248611900837\n",
      "\n",
      "Epoch: 70.0 | Loss: 1.5368193422800969\n",
      "\n",
      "Epoch: 70.1 | Loss: 1.2663596979389231\n",
      "\n",
      "Epoch: 70.2 | Loss: 1.0149854756517964\n",
      "\n",
      "Epoch: 70.3 | Loss: 0.7850948562848953\n",
      "\n",
      "Epoch: 70.4 | Loss: 0.5790939737572448\n",
      "\n",
      "Epoch: 71.0 | Loss: 1.4885844230369034\n",
      "\n",
      "Epoch: 71.1 | Loss: 1.2031851257365553\n",
      "\n",
      "Epoch: 71.2 | Loss: 0.9415445183174223\n",
      "\n",
      "Epoch: 71.3 | Loss: 0.7061649127558058\n",
      "\n",
      "Epoch: 71.4 | Loss: 0.4995825005852674\n",
      "\n",
      "Epoch: 72.0 | Loss: 1.4426699287662796\n",
      "\n",
      "Epoch: 72.1 | Loss: 1.1436811657671933\n",
      "\n",
      "Epoch: 72.2 | Loss: 0.8732395474659317\n",
      "\n",
      "Epoch: 72.3 | Loss: 0.6339346042507208\n",
      "\n",
      "Epoch: 72.4 | Loss: 0.42841158142961117\n",
      "\n",
      "Epoch: 73.0 | Loss: 1.3989706101733617\n",
      "\n",
      "Epoch: 73.1 | Loss: 1.0876478048341407\n",
      "\n",
      "Epoch: 73.2 | Loss: 0.8097488071135283\n",
      "\n",
      "Epoch: 73.3 | Loss: 0.5679346340620564\n",
      "\n",
      "Epoch: 73.4 | Loss: 0.36494005367895904\n",
      "\n",
      "Epoch: 74.0 | Loss: 1.357385306044111\n",
      "\n",
      "Epoch: 74.1 | Loss: 1.0348942526489138\n",
      "\n",
      "Epoch: 74.2 | Loss: 0.7507669075992567\n",
      "\n",
      "Epoch: 74.3 | Loss: 0.5077211809607375\n",
      "\n",
      "Epoch: 74.4 | Loss: 0.3085632508691194\n",
      "\n",
      "Epoch: 75.0 | Loss: 1.3178168440353129\n",
      "\n",
      "Epoch: 75.1 | Loss: 0.9852386778908817\n",
      "\n",
      "Epoch: 75.2 | Loss: 0.696004319492974\n",
      "\n",
      "Epoch: 75.3 | Loss: 0.452875072831228\n",
      "\n",
      "Epoch: 75.4 | Loss: 0.25871177708990367\n",
      "\n",
      "Epoch: 76.0 | Loss: 1.2801719390215291\n",
      "\n",
      "Epoch: 76.1 | Loss: 0.9385079388393565\n",
      "\n",
      "Epoch: 76.2 | Loss: 0.6451868581496832\n",
      "\n",
      "Epoch: 76.3 | Loss: 0.40300094626652744\n",
      "\n",
      "Epoch: 76.4 | Loss: 0.21485026213820418\n",
      "\n",
      "Epoch: 77.0 | Loss: 1.2443610893337145\n",
      "\n",
      "Epoch: 77.1 | Loss: 0.8945373095707003\n",
      "\n",
      "Epoch: 77.2 | Loss: 0.5980551608834848\n",
      "\n",
      "Epoch: 77.3 | Loss: 0.35772639539791284\n",
      "\n",
      "Epoch: 77.4 | Loss: 0.17647610231028302\n",
      "\n",
      "Epoch: 78.0 | Loss: 1.210298471244115\n",
      "\n",
      "Epoch: 78.1 | Loss: 0.8531702027346356\n",
      "\n",
      "Epoch: 78.2 | Loss: 0.5543641587502227\n",
      "\n",
      "Epoch: 78.3 | Loss: 0.3167011132345763\n",
      "\n",
      "Epoch: 78.4 | Loss: 0.14311819169951134\n",
      "\n",
      "Epoch: 79.0 | Loss: 1.1779018320638532\n",
      "\n",
      "Epoch: 79.1 | Loss: 0.8142578899291205\n",
      "\n",
      "Epoch: 79.2 | Loss: 0.5138825449157853\n",
      "\n",
      "Epoch: 79.3 | Loss: 0.2795960287469322\n",
      "\n",
      "Epoch: 79.4 | Loss: 0.114335648781363\n",
      "\n",
      "Epoch: 80.0 | Loss: 1.1470923822247447\n",
      "\n",
      "Epoch: 80.1 | Loss: 0.7776592206837608\n",
      "\n",
      "Epoch: 80.2 | Loss: 0.47639224155069704\n",
      "\n",
      "Epoch: 80.3 | Loss: 0.24610244284996996\n",
      "\n",
      "Epoch: 80.4 | Loss: 0.08971654293315251\n",
      "\n",
      "Epoch: 81.0 | Loss: 1.1177946867156092\n",
      "\n",
      "Epoch: 81.1 | Loss: 0.7432403410401218\n",
      "\n",
      "Epoch: 81.2 | Loss: 0.4416878671345679\n",
      "\n",
      "Epoch: 81.3 | Loss: 0.21593116633470208\n",
      "\n",
      "Epoch: 81.4 | Loss: 0.06887662535902069\n",
      "\n",
      "Epoch: 82.0 | Loss: 1.0899365562371721\n",
      "\n",
      "Epoch: 82.1 | Loss: 0.710874412684892\n",
      "\n",
      "Epoch: 82.2 | Loss: 0.40957620597922817\n",
      "\n",
      "Epoch: 82.3 | Loss: 0.18881166266115426\n",
      "\n",
      "Epoch: 82.4 | Loss: 0.05145806867827931\n",
      "\n",
      "Epoch: 83.0 | Loss: 1.0634489384289458\n",
      "\n",
      "Epoch: 83.1 | Loss: 0.6804413335507293\n",
      "\n",
      "Epoch: 83.2 | Loss: 0.37987568169011\n",
      "\n",
      "Epoch: 83.3 | Loss: 0.16449119837076423\n",
      "\n",
      "Epoch: 83.4 | Loss: 0.03712821919363056\n",
      "\n",
      "Epoch: 84.0 | Loss: 1.0382658095070456\n",
      "\n",
      "Epoch: 84.1 | Loss: 0.6518274607511393\n",
      "\n",
      "Epoch: 84.2 | Loss: 0.3524158361841579\n",
      "\n",
      "Epoch: 84.3 | Loss: 0.14273400370276149\n",
      "\n",
      "Epoch: 84.4 | Loss: 0.025578365591050523\n",
      "\n",
      "Epoch: 85.0 | Loss: 1.0143240666346418\n",
      "\n",
      "Epoch: 85.1 | Loss: 0.6249253366614069\n",
      "\n",
      "Epoch: 85.2 | Loss: 0.32703681577215926\n",
      "\n",
      "Epoch: 85.3 | Loss: 0.12332044581292911\n",
      "\n",
      "Epoch: 85.4 | Loss: 0.01652252754084237\n",
      "\n",
      "Epoch: 86.0 | Loss: 0.9915634213268868\n",
      "\n",
      "Epoch: 86.1 | Loss: 0.5996334188988831\n",
      "\n",
      "Epoch: 86.2 | Loss: 0.3035888656958721\n",
      "\n",
      "Epoch: 86.3 | Loss: 0.10604621679692867\n",
      "\n",
      "Epoch: 86.4 | Loss: 0.009696267374560839\n",
      "\n",
      "Epoch: 87.0 | Loss: 0.9699262941706267\n",
      "\n",
      "Epoch: 87.1 | Loss: 0.5758558148938477\n",
      "\n",
      "Epoch: 87.2 | Loss: 0.2819318343880393\n",
      "\n",
      "Epoch: 87.3 | Loss: 0.09072153851797532\n",
      "\n",
      "Epoch: 87.4 | Loss: 0.004855527709770378\n",
      "\n",
      "Epoch: 88.0 | Loss: 0.9493577111162586\n",
      "\n",
      "Epoch: 88.1 | Loss: 0.5535020216780804\n",
      "\n",
      "Epoch: 88.2 | Loss: 0.2619346885981764\n",
      "\n",
      "Epoch: 88.3 | Loss: 0.077170386032587\n",
      "\n",
      "Epoch: 88.4 | Loss: 0.0017754975881634973\n",
      "\n",
      "Epoch: 89.0 | Loss: 0.9298052015752992\n",
      "\n",
      "Epoch: 89.1 | Loss: 0.5324866714528631\n",
      "\n",
      "Epoch: 89.2 | Loss: 0.24347504040069629\n",
      "\n",
      "Epoch: 89.3 | Loss: 0.0652297312013502\n",
      "\n",
      "Epoch: 89.4 | Loss: 0.00024950938616096223\n",
      "\n",
      "Epoch: 90.0 | Loss: 0.9112186985329879\n",
      "\n",
      "Epoch: 90.1 | Loss: 0.5127292834326785\n",
      "\n",
      "Epoch: 90.2 | Loss: 0.22643868697592429\n",
      "\n",
      "Epoch: 90.3 | Loss: 0.054748807866410544\n",
      "\n",
      "Epoch: 90.4 | Loss: 8.796845400563937e-05\n",
      "\n",
      "Epoch: 91.0 | Loss: 0.8935504408608521\n",
      "\n",
      "Epoch: 91.1 | Loss: 0.4941540223957612\n",
      "\n",
      "Epoch: 91.2 | Loss: 0.21071916393033038\n",
      "\n",
      "Epoch: 91.3 | Loss: 0.04558839977566268\n",
      "\n",
      "Epoch: 91.4 | Loss: 0.0011173171425178778\n",
      "\n",
      "Epoch: 92.0 | Loss: 0.8767548779900254\n",
      "\n",
      "Epoch: 92.1 | Loss: 0.47668946430887366\n",
      "\n",
      "Epoch: 92.2 | Loss: 0.19621731280103477\n",
      "\n",
      "Epoch: 92.3 | Loss: 0.03762015223744422\n",
      "\n",
      "Epoch: 92.4 | Loss: 0.0031790345885797516\n",
      "\n",
      "Epoch: 93.0 | Loss: 0.860788577082371\n",
      "\n",
      "Epoch: 93.1 | Loss: 0.4602683693317037\n",
      "\n",
      "Epoch: 93.2 | Loss: 0.18284086327223525\n",
      "\n",
      "Epoch: 93.3 | Loss: 0.030725908300210727\n",
      "\n",
      "Epoch: 93.4 | Loss: 0.00612867335318658\n",
      "\n",
      "Epoch: 94.0 | Loss: 0.8456101328135744\n",
      "\n",
      "Epoch: 94.1 | Loss: 0.4448274624465208\n",
      "\n",
      "Epoch: 94.2 | Loss: 0.17050403051871765\n",
      "\n",
      "Epoch: 94.3 | Loss: 0.024797070070577076\n",
      "\n",
      "Epoch: 94.4 | Loss: 0.009834933741336078\n",
      "\n",
      "Epoch: 95.0 | Loss: 0.8311800798601853\n",
      "\n",
      "Epoch: 95.1 | Loss: 0.4303072219018125\n",
      "\n",
      "Epoch: 95.2 | Loss: 0.15912712798443523\n",
      "\n",
      "Epoch: 95.3 | Loss: 0.019733985611331988\n",
      "\n",
      "Epoch: 95.4 | Loss: 0.014178776382547015\n",
      "\n",
      "Epoch: 96.0 | Loss: 0.8174608081616676\n",
      "\n",
      "Epoch: 96.1 | Loss: 0.41665167560465777\n",
      "\n",
      "Epoch: 96.2 | Loss: 0.1486361958032482\n",
      "\n",
      "Epoch: 96.3 | Loss: 0.015445361699307493\n",
      "\n",
      "Epoch: 96.4 | Loss: 0.019052573415530433\n",
      "\n",
      "Epoch: 97.0 | Loss: 0.8044164810086362\n",
      "\n",
      "Epoch: 97.1 | Loss: 0.40380820554601\n",
      "\n",
      "Epoch: 97.2 | Loss: 0.1389626449743182\n",
      "\n",
      "Epoch: 97.3 | Loss: 0.01184770257203472\n",
      "\n",
      "Epoch: 97.4 | Loss: 0.02435929840131449\n",
      "\n",
      "Epoch: 98.0 | Loss: 0.792012955989943\n",
      "\n",
      "Epoch: 98.1 | Loss: 0.3917273602959026\n",
      "\n",
      "Epoch: 98.2 | Loss: 0.13004291731691553\n",
      "\n",
      "Epoch: 98.3 | Loss: 0.008864774652235962\n",
      "\n",
      "Epoch: 98.4 | Loss: 0.03001175488651455\n",
      "\n",
      "Epoch: 99.0 | Loss: 0.7802177088140673\n",
      "\n",
      "Epoch: 99.1 | Loss: 0.38036267556222064\n",
      "\n",
      "Epoch: 99.2 | Loss: 0.1218181611486906\n",
      "\n",
      "Epoch: 99.3 | Loss: 0.006427097110850201\n",
      "\n",
      "Epoch: 99.4 | Loss: 0.03593184335275492\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0.5 * x1 + 1.5 * x2 + 3 for x1, x2 in X])\n",
    "\n",
    "model = Model(DenseLayer, MSE, Adam)\n",
    "model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(4.5), array([4.5]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([0.5, 1.5])\n",
    "x = np.array([[1,2], [2,3], [4,5]])\n",
    "y = np.array([0.5 * x1 + 1.5 * x2 + 3 for x1, x2 in x])\n",
    "b =np.array([1])\n",
    "\n",
    "(np.dot(w, x.T) + b)[0], np.dot(w, x[0].T) +b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
