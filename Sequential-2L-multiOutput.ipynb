{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- add mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.01, beta_1=0.85, beta_2=0.99):\n",
    "        self.lr = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "\n",
    "        self.m_prev = 0\n",
    "        self.v_prev = 0\n",
    "\n",
    "    def adam(self, gradients: list, epoch):\n",
    "        self.prev_m = self.m\n",
    "        self.prev_v = self.v\n",
    "\n",
    "        self.m = self.beta_1 * self.prev_m + (1 - self.beta_1) * gradients\n",
    "        self.v = self.beta_2 * self.prev_v + (1 - self.beta_2) * (gradients**2)\n",
    "\n",
    "        m_hat = self.m / (1 - self.beta_1 ** (epoch + 1))\n",
    "        v_hat = self.v / (1 - self.beta_2 ** (epoch + 1))\n",
    "\n",
    "        learning_rate = self.lr / (np.sqrt(v_hat) + 1e-8)\n",
    "        return learning_rate * m_hat\n",
    "\n",
    "    def step(self, parameters: list, gradients: list, epoch):\n",
    "        parameters = np.array(parameters).flatten()\n",
    "        gradients = np.array(gradients).flatten()\n",
    "\n",
    "        new_parameters = []\n",
    "        for param, gradient in zip(parameters, gradients):\n",
    "            update = self.adam(gradient, epoch)\n",
    "            param -= update\n",
    "            new_parameters.append(param)\n",
    "        return np.array(new_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def calculate_loss(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def calculate_gradient_coeff(self, y_pred, y_true, X):\n",
    "        errors = y_pred - y_true\n",
    "        return (2 / len(y_true)) * np.dot(errors, X)\n",
    "\n",
    "    def calculate_gradient_bias(self, y_pred, y_true):\n",
    "        errors = y_pred - y_true\n",
    "        return (2 / len(y_true)) * np.sum(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, let's manually define the output size of the Dense layer, but in the future, we can infer it from the next layer\n",
    "class Dense:\n",
    "    def __init__(self, input_size, output_size, verbose=True):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.initialize_params()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return np.dot(self.weights, X.T) + self.bias\n",
    "\n",
    "    def initialize_params(self):\n",
    "        # new axis is because the dot product of x[100,3] and weights[3,2] will result in a shape of [100,2], but we have bias of shape [2]\n",
    "        # so we need to add a new axis to the bias to make it [2,1] so that we can add it to the dot product result\n",
    "        self.weights = np.random.randn(self.output_size, self.input_size)\n",
    "        self.bias = np.random.randn(self.output_size, 1)\n",
    "        if self.verbose:\n",
    "            print(f\"Weight shape: {self.weights.shape}\")\n",
    "            print(f\"Bias shape: {self.bias.shape}\")\n",
    "            print(\"Initialized weights:\", self.weights)\n",
    "            print(\"Initialized bias:\", self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer  # not initialized\n",
    "        self.initiated = False\n",
    "        self.compiled = False\n",
    "\n",
    "    def compile(self, loss, optimizer):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.compiled = True\n",
    "\n",
    "    def _forward(self, x):\n",
    "        if not self.initiated:\n",
    "            raise Exception(\"Model not initiated - call `fit()` method first\")\n",
    "\n",
    "        return self.layer.forward(x)\n",
    "\n",
    "    def _backward(self, y_pred, y_real, X):\n",
    "        if not self.initiated:\n",
    "            raise Exception(\"Model not initiated - call `fit()` method first\")\n",
    "\n",
    "        gradient_coeff = self.loss.calculate_gradient_coeff(y_pred, y_real, X)\n",
    "        gradient_bias = self.loss.calculate_gradient_bias(y_pred, y_real)\n",
    "\n",
    "        return gradient_coeff, gradient_bias\n",
    "\n",
    "    def fit(self, X, y, epochs=100):\n",
    "        if not self.compiled:\n",
    "            raise Exception(\"Model not compiled - call `compile()` method first\")\n",
    "\n",
    "        input_dim = X.shape[-1]\n",
    "        output_dim = y.shape[-1]\n",
    "        self.layer = self.layer(\n",
    "            input_size=input_dim, output_size=output_dim, verbose=True\n",
    "        )\n",
    "        self.initiated = True\n",
    "        \n",
    "        \n",
    "        loss = None\n",
    "        print(\"\\n------ Model initiated ------\")\n",
    "        for epoch in range(epochs):\n",
    "            # we need to find gradients of weights for each output separately\n",
    "            # so we need to iterate over each output\n",
    "            # for now let's use the batch (full) gradient descent\n",
    "\n",
    "            updated_weights = []\n",
    "            updated_bias = []\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch} | Loss: {loss}\")\n",
    "            for i in range(output_dim):\n",
    "                # print(f\"\\nOutput {i}\")\n",
    "                weights = self.layer.weights[i, :]\n",
    "                bias = self.layer.bias[i]\n",
    "                # print(f\"Weights: {weights}\")\n",
    "                # print(f\"Bias: {bias}\")\n",
    "                \n",
    "                y_pred = self._forward(X)[i, :]\n",
    "                y_real = y[:, i]\n",
    "                # print(f\"X shape: {X.shape}\")\n",
    "                # print(f\"y_pred shape: {y_pred.shape}\")\n",
    "                # print(f\"y_real shape: {y_real.shape}\")\n",
    "                \n",
    "                loss = self.loss.calculate_loss(y_pred, y_real)\n",
    "                \n",
    "                \n",
    "                grad_coeff, grad_bias = self._backward(y_pred, y_real, X)\n",
    "                # print(f\"Gradient coeff: {grad_coeff}\")\n",
    "                # print(f\"Gradient bias: {grad_bias}\")\n",
    "                \n",
    "                new_weights = self.optimizer.step(weights, grad_coeff, epoch)\n",
    "                new_bias = self.optimizer.step(bias, grad_bias, epoch)\n",
    "                \n",
    "                updated_weights.append(new_weights)\n",
    "                updated_bias.append(new_bias)\n",
    "                \n",
    "            self.layer.weights = np.array(updated_weights)\n",
    "            self.layer.bias = np.array(updated_bias)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3) (100, 2)\n",
      "Weight shape: (2, 3)\n",
      "Bias shape: (2, 1)\n",
      "Initialized weights: [[-0.71465269 -0.1594483   1.00182898]\n",
      " [-0.42849634  0.19283412 -0.33051069]]\n",
      "Initialized bias: [[ 1.22796239]\n",
      " [-0.21034963]]\n",
      "\n",
      "------ Model initiated ------\n",
      "\n",
      "Epoch 0 | Loss: None\n",
      "\n",
      "Epoch 1 | Loss: 21.517050787004056\n",
      "\n",
      "Epoch 2 | Loss: 21.376553984476736\n",
      "\n",
      "Epoch 3 | Loss: 21.279441182484216\n",
      "\n",
      "Epoch 4 | Loss: 21.204939542895918\n",
      "\n",
      "Epoch 5 | Loss: 21.14339593540862\n",
      "\n",
      "Epoch 6 | Loss: 21.089805657700207\n",
      "\n",
      "Epoch 7 | Loss: 21.041392915246675\n",
      "\n",
      "Epoch 8 | Loss: 20.996508484408213\n",
      "\n",
      "Epoch 9 | Loss: 20.95410925882116\n",
      "\n",
      "Epoch 10 | Loss: 20.913499364671033\n",
      "\n",
      "Epoch 11 | Loss: 20.87419376296551\n",
      "\n",
      "Epoch 12 | Loss: 20.835842181647454\n",
      "\n",
      "Epoch 13 | Loss: 20.798184387161708\n",
      "\n",
      "Epoch 14 | Loss: 20.76102260497095\n",
      "\n",
      "Epoch 15 | Loss: 20.724203789926975\n",
      "\n",
      "Epoch 16 | Loss: 20.687607807343213\n",
      "\n",
      "Epoch 17 | Loss: 20.65113930153176\n",
      "\n",
      "Epoch 18 | Loss: 20.61472194493761\n",
      "\n",
      "Epoch 19 | Loss: 20.578294271217345\n",
      "\n",
      "Epoch 20 | Loss: 20.541806590701537\n",
      "\n",
      "Epoch 21 | Loss: 20.50521866329656\n",
      "\n",
      "Epoch 22 | Loss: 20.46849791287407\n",
      "\n",
      "Epoch 23 | Loss: 20.431618036318017\n",
      "\n",
      "Epoch 24 | Loss: 20.39455790531828\n",
      "\n",
      "Epoch 25 | Loss: 20.35730068883761\n",
      "\n",
      "Epoch 26 | Loss: 20.319833144393826\n",
      "\n",
      "Epoch 27 | Loss: 20.282145040244767\n",
      "\n",
      "Epoch 28 | Loss: 20.244228680343635\n",
      "\n",
      "Epoch 29 | Loss: 20.206078510897214\n",
      "\n",
      "Epoch 30 | Loss: 20.16769079239101\n",
      "\n",
      "Epoch 31 | Loss: 20.129063324629026\n",
      "\n",
      "Epoch 32 | Loss: 20.090195215067965\n",
      "\n",
      "Epoch 33 | Loss: 20.051086682776283\n",
      "\n",
      "Epoch 34 | Loss: 20.011738891905967\n",
      "\n",
      "Epoch 35 | Loss: 19.972153809761153\n",
      "\n",
      "Epoch 36 | Loss: 19.932334085476008\n",
      "\n",
      "Epoch 37 | Loss: 19.892282946042354\n",
      "\n",
      "Epoch 38 | Loss: 19.852004107004046\n",
      "\n",
      "Epoch 39 | Loss: 19.811501695595638\n",
      "\n",
      "Epoch 40 | Loss: 19.7707801844741\n",
      "\n",
      "Epoch 41 | Loss: 19.729844334493606\n",
      "\n",
      "Epoch 42 | Loss: 19.68869914522041\n",
      "\n",
      "Epoch 43 | Loss: 19.64734981208745\n",
      "\n",
      "Epoch 44 | Loss: 19.605801689257067\n",
      "\n",
      "Epoch 45 | Loss: 19.564060257399984\n",
      "\n",
      "Epoch 46 | Loss: 19.522131095716194\n",
      "\n",
      "Epoch 47 | Loss: 19.480019857621883\n",
      "\n",
      "Epoch 48 | Loss: 19.43773224960961\n",
      "\n",
      "Epoch 49 | Loss: 19.395274012859407\n",
      "\n",
      "Epoch 50 | Loss: 19.352650907238026\n",
      "\n",
      "Epoch 51 | Loss: 19.309868697374437\n",
      "\n",
      "Epoch 52 | Loss: 19.266933140542843\n",
      "\n",
      "Epoch 53 | Loss: 19.223849976121578\n",
      "\n",
      "Epoch 54 | Loss: 19.180624916427746\n",
      "\n",
      "Epoch 55 | Loss: 19.137263638754575\n",
      "\n",
      "Epoch 56 | Loss: 19.09377177846173\n",
      "\n",
      "Epoch 57 | Loss: 19.050154922988657\n",
      "\n",
      "Epoch 58 | Loss: 19.006418606678377\n",
      "\n",
      "Epoch 59 | Loss: 18.962568306313596\n",
      "\n",
      "Epoch 60 | Loss: 18.918609437280015\n",
      "\n",
      "Epoch 61 | Loss: 18.874547350282466\n",
      "\n",
      "Epoch 62 | Loss: 18.830387328549044\n",
      "\n",
      "Epoch 63 | Loss: 18.786134585466474\n",
      "\n",
      "Epoch 64 | Loss: 18.74179426259713\n",
      "\n",
      "Epoch 65 | Loss: 18.697371428034163\n",
      "\n",
      "Epoch 66 | Loss: 18.652871075056456\n",
      "\n",
      "Epoch 67 | Loss: 18.608298121049746\n",
      "\n",
      "Epoch 68 | Loss: 18.56365740666433\n",
      "\n",
      "Epoch 69 | Loss: 18.518953695182958\n",
      "\n",
      "Epoch 70 | Loss: 18.474191672075918\n",
      "\n",
      "Epoch 71 | Loss: 18.429375944722675\n",
      "\n",
      "Epoch 72 | Loss: 18.384511042281773\n",
      "\n",
      "Epoch 73 | Loss: 18.33960141569279\n",
      "\n",
      "Epoch 74 | Loss: 18.29465143779588\n",
      "\n",
      "Epoch 75 | Loss: 18.24966540355596\n",
      "\n",
      "Epoch 76 | Loss: 18.204647530379958\n",
      "\n",
      "Epoch 77 | Loss: 18.159601958516763\n",
      "\n",
      "Epoch 78 | Loss: 18.114532751530618\n",
      "\n",
      "Epoch 79 | Loss: 18.069443896839523\n",
      "\n",
      "Epoch 80 | Loss: 18.02433930631121\n",
      "\n",
      "Epoch 81 | Loss: 17.979222816909836\n",
      "\n",
      "Epoch 82 | Loss: 17.934098191387317\n",
      "\n",
      "Epoch 83 | Loss: 17.888969119013776\n",
      "\n",
      "Epoch 84 | Loss: 17.84383921634208\n",
      "\n",
      "Epoch 85 | Loss: 17.798712028001955\n",
      "\n",
      "Epoch 86 | Loss: 17.75359102751966\n",
      "\n",
      "Epoch 87 | Loss: 17.708479618159394\n",
      "\n",
      "Epoch 88 | Loss: 17.663381133783115\n",
      "\n",
      "Epoch 89 | Loss: 17.618298839725796\n",
      "\n",
      "Epoch 90 | Loss: 17.57323593368324\n",
      "\n",
      "Epoch 91 | Loss: 17.528195546610085\n",
      "\n",
      "Epoch 92 | Loss: 17.483180743625564\n",
      "\n",
      "Epoch 93 | Loss: 17.438194524925144\n",
      "\n",
      "Epoch 94 | Loss: 17.393239826696014\n",
      "\n",
      "Epoch 95 | Loss: 17.34831952203491\n",
      "\n",
      "Epoch 96 | Loss: 17.30343642186657\n",
      "\n",
      "Epoch 97 | Loss: 17.258593275861625\n",
      "\n",
      "Epoch 98 | Loss: 17.21379277335252\n",
      "\n",
      "Epoch 99 | Loss: 17.169037544246525\n",
      "\n",
      "Epoch 100 | Loss: 17.124330159934647\n",
      "\n",
      "Epoch 101 | Loss: 17.07967313419572\n",
      "\n",
      "Epoch 102 | Loss: 17.035068924094713\n",
      "\n",
      "Epoch 103 | Loss: 16.990519930874697\n",
      "\n",
      "Epoch 104 | Loss: 16.946028500841663\n",
      "\n",
      "Epoch 105 | Loss: 16.90159692624179\n",
      "\n",
      "Epoch 106 | Loss: 16.857227446130548\n",
      "\n",
      "Epoch 107 | Loss: 16.812922247233278\n",
      "\n",
      "Epoch 108 | Loss: 16.768683464796798\n",
      "\n",
      "Epoch 109 | Loss: 16.724513183431746\n",
      "\n",
      "Epoch 110 | Loss: 16.68041343794543\n",
      "\n",
      "Epoch 111 | Loss: 16.6363862141648\n",
      "\n",
      "Epoch 112 | Loss: 16.592433449749528\n",
      "\n",
      "Epoch 113 | Loss: 16.548557034994836\n",
      "\n",
      "Epoch 114 | Loss: 16.504758813624143\n",
      "\n",
      "Epoch 115 | Loss: 16.46104058357122\n",
      "\n",
      "Epoch 116 | Loss: 16.417404097751955\n",
      "\n",
      "Epoch 117 | Loss: 16.373851064825583\n",
      "\n",
      "Epoch 118 | Loss: 16.33038314994534\n",
      "\n",
      "Epoch 119 | Loss: 16.287001975498587\n",
      "\n",
      "Epoch 120 | Loss: 16.243709121836385\n",
      "\n",
      "Epoch 121 | Loss: 16.20050612799253\n",
      "\n",
      "Epoch 122 | Loss: 16.157394492392108\n",
      "\n",
      "Epoch 123 | Loss: 16.114375673549613\n",
      "\n",
      "Epoch 124 | Loss: 16.071451090756703\n",
      "\n",
      "Epoch 125 | Loss: 16.028622124759632\n",
      "\n",
      "Epoch 126 | Loss: 15.98589011842649\n",
      "\n",
      "Epoch 127 | Loss: 15.943256377404298\n",
      "\n",
      "Epoch 128 | Loss: 15.900722170766073\n",
      "\n",
      "Epoch 129 | Loss: 15.858288731648015\n",
      "\n",
      "Epoch 130 | Loss: 15.81595725787678\n",
      "\n",
      "Epoch 131 | Loss: 15.77372891258716\n",
      "\n",
      "Epoch 132 | Loss: 15.731604824830097\n",
      "\n",
      "Epoch 133 | Loss: 15.689586090171282\n",
      "\n",
      "Epoch 134 | Loss: 15.647673771280386\n",
      "\n",
      "Epoch 135 | Loss: 15.605868898511073\n",
      "\n",
      "Epoch 136 | Loss: 15.564172470471972\n",
      "\n",
      "Epoch 137 | Loss: 15.522585454588619\n",
      "\n",
      "Epoch 138 | Loss: 15.481108787656643\n",
      "\n",
      "Epoch 139 | Loss: 15.439743376386167\n",
      "\n",
      "Epoch 140 | Loss: 15.39849009793775\n",
      "\n",
      "Epoch 141 | Loss: 15.3573498004498\n",
      "\n",
      "Epoch 142 | Loss: 15.316323303557713\n",
      "\n",
      "Epoch 143 | Loss: 15.275411398904826\n",
      "\n",
      "Epoch 144 | Loss: 15.23461485064535\n",
      "\n",
      "Epoch 145 | Loss: 15.193934395939328\n",
      "\n",
      "Epoch 146 | Loss: 15.153370745439831\n",
      "\n",
      "Epoch 147 | Loss: 15.112924583772497\n",
      "\n",
      "Epoch 148 | Loss: 15.072596570007478\n",
      "\n",
      "Epoch 149 | Loss: 15.032387338124021\n",
      "\n",
      "Epoch 150 | Loss: 14.992297497467675\n",
      "\n",
      "Epoch 151 | Loss: 14.952327633200404\n",
      "\n",
      "Epoch 152 | Loss: 14.912478306743546\n",
      "\n",
      "Epoch 153 | Loss: 14.87275005621391\n",
      "\n",
      "Epoch 154 | Loss: 14.83314339685293\n",
      "\n",
      "Epoch 155 | Loss: 14.793658821449236\n",
      "\n",
      "Epoch 156 | Loss: 14.754296800754476\n",
      "\n",
      "Epoch 157 | Loss: 14.715057783892728\n",
      "\n",
      "Epoch 158 | Loss: 14.675942198763485\n",
      "\n",
      "Epoch 159 | Loss: 14.63695045243833\n",
      "\n",
      "Epoch 160 | Loss: 14.598082931551474\n",
      "\n",
      "Epoch 161 | Loss: 14.55934000268415\n",
      "\n",
      "Epoch 162 | Loss: 14.520722012743061\n",
      "\n",
      "Epoch 163 | Loss: 14.482229289332915\n",
      "\n",
      "Epoch 164 | Loss: 14.44386214112318\n",
      "\n",
      "Epoch 165 | Loss: 14.405620858209145\n",
      "\n",
      "Epoch 166 | Loss: 14.367505712467345\n",
      "\n",
      "Epoch 167 | Loss: 14.329516957905494\n",
      "\n",
      "Epoch 168 | Loss: 14.29165483100697\n",
      "\n",
      "Epoch 169 | Loss: 14.253919551069982\n",
      "\n",
      "Epoch 170 | Loss: 14.216311320541461\n",
      "\n",
      "Epoch 171 | Loss: 14.178830325345757\n",
      "\n",
      "Epoch 172 | Loss: 14.141476735208306\n",
      "\n",
      "Epoch 173 | Loss: 14.104250703974204\n",
      "\n",
      "Epoch 174 | Loss: 14.067152369921912\n",
      "\n",
      "Epoch 175 | Loss: 14.030181856072065\n",
      "\n",
      "Epoch 176 | Loss: 13.993339270491514\n",
      "\n",
      "Epoch 177 | Loss: 13.95662470659268\n",
      "\n",
      "Epoch 178 | Loss: 13.920038243428198\n",
      "\n",
      "Epoch 179 | Loss: 13.883579945981099\n",
      "\n",
      "Epoch 180 | Loss: 13.847249865450422\n",
      "\n",
      "Epoch 181 | Loss: 13.81104803953242\n",
      "\n",
      "Epoch 182 | Loss: 13.774974492697424\n",
      "\n",
      "Epoch 183 | Loss: 13.73902923646237\n",
      "\n",
      "Epoch 184 | Loss: 13.703212269659153\n",
      "\n",
      "Epoch 185 | Loss: 13.667523578698756\n",
      "\n",
      "Epoch 186 | Loss: 13.631963137831317\n",
      "\n",
      "Epoch 187 | Loss: 13.596530909402132\n",
      "\n",
      "Epoch 188 | Loss: 13.561226844103668\n",
      "\n",
      "Epoch 189 | Loss: 13.526050881223664\n",
      "\n",
      "Epoch 190 | Loss: 13.49100294888935\n",
      "\n",
      "Epoch 191 | Loss: 13.456082964307875\n",
      "\n",
      "Epoch 192 | Loss: 13.421290834002932\n",
      "\n",
      "Epoch 193 | Loss: 13.386626454047741\n",
      "\n",
      "Epoch 194 | Loss: 13.352089710294317\n",
      "\n",
      "Epoch 195 | Loss: 13.317680478599202\n",
      "\n",
      "Epoch 196 | Loss: 13.283398625045606\n",
      "\n",
      "Epoch 197 | Loss: 13.249244006162062\n",
      "\n",
      "Epoch 198 | Loss: 13.21521646913765\n",
      "\n",
      "Epoch 199 | Loss: 13.181315852033807\n",
      "\n",
      "Epoch 200 | Loss: 13.147541983992799\n",
      "\n",
      "Epoch 201 | Loss: 13.113894685442869\n",
      "\n",
      "Epoch 202 | Loss: 13.080373768300179\n",
      "\n",
      "Epoch 203 | Loss: 13.046979036167457\n",
      "\n",
      "Epoch 204 | Loss: 13.013710284529571\n",
      "\n",
      "Epoch 205 | Loss: 12.980567300945902\n",
      "\n",
      "Epoch 206 | Loss: 12.947549865239711\n",
      "\n",
      "Epoch 207 | Loss: 12.914657749684402\n",
      "\n",
      "Epoch 208 | Loss: 12.881890719186835\n",
      "\n",
      "Epoch 209 | Loss: 12.849248531467659\n",
      "\n",
      "Epoch 210 | Loss: 12.81673093723877\n",
      "\n",
      "Epoch 211 | Loss: 12.784337680377861\n",
      "\n",
      "Epoch 212 | Loss: 12.752068498100144\n",
      "\n",
      "Epoch 213 | Loss: 12.719923121127303\n",
      "\n",
      "Epoch 214 | Loss: 12.687901273853656\n",
      "\n",
      "Epoch 215 | Loss: 12.656002674509628\n",
      "\n",
      "Epoch 216 | Loss: 12.624227035322535\n",
      "\n",
      "Epoch 217 | Loss: 12.592574062674721\n",
      "\n",
      "Epoch 218 | Loss: 12.561043457259077\n",
      "\n",
      "Epoch 219 | Loss: 12.529634914231993\n",
      "\n",
      "Epoch 220 | Loss: 12.498348123363803\n",
      "\n",
      "Epoch 221 | Loss: 12.467182769186673\n",
      "\n",
      "Epoch 222 | Loss: 12.436138531140061\n",
      "\n",
      "Epoch 223 | Loss: 12.405215083713722\n",
      "\n",
      "Epoch 224 | Loss: 12.374412096588312\n",
      "\n",
      "Epoch 225 | Loss: 12.34372923477364\n",
      "\n",
      "Epoch 226 | Loss: 12.313166158744542\n",
      "\n",
      "Epoch 227 | Loss: 12.282722524574508\n",
      "\n",
      "Epoch 228 | Loss: 12.252397984066977\n",
      "\n",
      "Epoch 229 | Loss: 12.222192184884404\n",
      "\n",
      "Epoch 230 | Loss: 12.192104770675169\n",
      "\n",
      "Epoch 231 | Loss: 12.162135381198205\n",
      "\n",
      "Epoch 232 | Loss: 12.13228365244554\n",
      "\n",
      "Epoch 233 | Loss: 12.102549216762732\n",
      "\n",
      "Epoch 234 | Loss: 12.072931702967121\n",
      "\n",
      "Epoch 235 | Loss: 12.04343073646412\n",
      "\n",
      "Epoch 236 | Loss: 12.014045939361388\n",
      "\n",
      "Epoch 237 | Loss: 11.984776930581031\n",
      "\n",
      "Epoch 238 | Loss: 11.955623325969817\n",
      "\n",
      "Epoch 239 | Loss: 11.926584738407442\n",
      "\n",
      "Epoch 240 | Loss: 11.89766077791282\n",
      "\n",
      "Epoch 241 | Loss: 11.868851051748555\n",
      "\n",
      "Epoch 242 | Loss: 11.84015516452347\n",
      "\n",
      "Epoch 243 | Loss: 11.811572718293299\n",
      "\n",
      "Epoch 244 | Loss: 11.783103312659557\n",
      "\n",
      "Epoch 245 | Loss: 11.75474654486666\n",
      "\n",
      "Epoch 246 | Loss: 11.726502009897183\n",
      "\n",
      "Epoch 247 | Loss: 11.698369300565437\n",
      "\n",
      "Epoch 248 | Loss: 11.670348007609292\n",
      "\n",
      "Epoch 249 | Loss: 11.642437719780302\n",
      "\n",
      "Epoch 250 | Loss: 11.614638023932175\n",
      "\n",
      "Epoch 251 | Loss: 11.58694850510755\n",
      "\n",
      "Epoch 252 | Loss: 11.559368746623187\n",
      "\n",
      "Epoch 253 | Loss: 11.531898330153515\n",
      "\n",
      "Epoch 254 | Loss: 11.50453683581261\n",
      "\n",
      "Epoch 255 | Loss: 11.477283842234636\n",
      "\n",
      "Epoch 256 | Loss: 11.450138926652675\n",
      "\n",
      "Epoch 257 | Loss: 11.423101664976148\n",
      "\n",
      "Epoch 258 | Loss: 11.396171631866627\n",
      "\n",
      "Epoch 259 | Loss: 11.369348400812239\n",
      "\n",
      "Epoch 260 | Loss: 11.342631544200628\n",
      "\n",
      "Epoch 261 | Loss: 11.316020633390417\n",
      "\n",
      "Epoch 262 | Loss: 11.289515238781338\n",
      "\n",
      "Epoch 263 | Loss: 11.263114929882907\n",
      "\n",
      "Epoch 264 | Loss: 11.236819275381762\n",
      "\n",
      "Epoch 265 | Loss: 11.21062784320766\n",
      "\n",
      "Epoch 266 | Loss: 11.184540200598091\n",
      "\n",
      "Epoch 267 | Loss: 11.15855591416166\n",
      "\n",
      "Epoch 268 | Loss: 11.132674549940084\n",
      "\n",
      "Epoch 269 | Loss: 11.106895673468987\n",
      "\n",
      "Epoch 270 | Loss: 11.08121884983744\n",
      "\n",
      "Epoch 271 | Loss: 11.05564364374618\n",
      "\n",
      "Epoch 272 | Loss: 11.030169619564749\n",
      "\n",
      "Epoch 273 | Loss: 11.004796341387303\n",
      "\n",
      "Epoch 274 | Loss: 10.979523373087323\n",
      "\n",
      "Epoch 275 | Loss: 10.954350278371129\n",
      "\n",
      "Epoch 276 | Loss: 10.929276620830256\n",
      "\n",
      "Epoch 277 | Loss: 10.904301963992697\n",
      "\n",
      "Epoch 278 | Loss: 10.879425871373053\n",
      "\n",
      "Epoch 279 | Loss: 10.854647906521556\n",
      "\n",
      "Epoch 280 | Loss: 10.82996763307205\n",
      "\n",
      "Epoch 281 | Loss: 10.805384614788885\n",
      "\n",
      "Epoch 282 | Loss: 10.780898415612779\n",
      "\n",
      "Epoch 283 | Loss: 10.756508599705647\n",
      "\n",
      "Epoch 284 | Loss: 10.732214731494437\n",
      "\n",
      "Epoch 285 | Loss: 10.708016375713926\n",
      "\n",
      "Epoch 286 | Loss: 10.683913097448583\n",
      "\n",
      "Epoch 287 | Loss: 10.659904462173431\n",
      "\n",
      "Epoch 288 | Loss: 10.635990035793995\n",
      "\n",
      "Epoch 289 | Loss: 10.612169384685268\n",
      "\n",
      "Epoch 290 | Loss: 10.588442075729803\n",
      "\n",
      "Epoch 291 | Loss: 10.564807676354866\n",
      "\n",
      "Epoch 292 | Loss: 10.54126575456871\n",
      "\n",
      "Epoch 293 | Loss: 10.517815878996002\n",
      "\n",
      "Epoch 294 | Loss: 10.49445761891232\n",
      "\n",
      "Epoch 295 | Loss: 10.47119054427786\n",
      "\n",
      "Epoch 296 | Loss: 10.448014225770299\n",
      "\n",
      "Epoch 297 | Loss: 10.424928234816802\n",
      "\n",
      "Epoch 298 | Loss: 10.401932143625277\n",
      "\n",
      "Epoch 299 | Loss: 10.379025525214772\n",
      "\n",
      "Epoch 300 | Loss: 10.356207953445162\n",
      "\n",
      "Epoch 301 | Loss: 10.333479003046001\n",
      "\n",
      "Epoch 302 | Loss: 10.310838249644664\n",
      "\n",
      "Epoch 303 | Loss: 10.288285269793727\n",
      "\n",
      "Epoch 304 | Loss: 10.265819640997625\n",
      "\n",
      "Epoch 305 | Loss: 10.243440941738568\n",
      "\n",
      "Epoch 306 | Loss: 10.221148751501813\n",
      "\n",
      "Epoch 307 | Loss: 10.198942650800163\n",
      "\n",
      "Epoch 308 | Loss: 10.176822221197831\n",
      "\n",
      "Epoch 309 | Loss: 10.154787045333647\n",
      "\n",
      "Epoch 310 | Loss: 10.13283670694356\n",
      "\n",
      "Epoch 311 | Loss: 10.110970790882531\n",
      "\n",
      "Epoch 312 | Loss: 10.08918888314578\n",
      "\n",
      "Epoch 313 | Loss: 10.067490570889396\n",
      "\n",
      "Epoch 314 | Loss: 10.045875442450349\n",
      "\n",
      "Epoch 315 | Loss: 10.024343087365903\n",
      "\n",
      "Epoch 316 | Loss: 10.002893096392413\n",
      "\n",
      "Epoch 317 | Loss: 9.98152506152357\n",
      "\n",
      "Epoch 318 | Loss: 9.960238576008036\n",
      "\n",
      "Epoch 319 | Loss: 9.939033234366583\n",
      "\n",
      "Epoch 320 | Loss: 9.917908632408592\n",
      "\n",
      "Epoch 321 | Loss: 9.896864367248101\n",
      "\n",
      "Epoch 322 | Loss: 9.87590003731926\n",
      "\n",
      "Epoch 323 | Loss: 9.855015242391286\n",
      "\n",
      "Epoch 324 | Loss: 9.834209583582913\n",
      "\n",
      "Epoch 325 | Loss: 9.813482663376329\n",
      "\n",
      "Epoch 326 | Loss: 9.792834085630616\n",
      "\n",
      "Epoch 327 | Loss: 9.77226345559474\n",
      "\n",
      "Epoch 328 | Loss: 9.751770379920002\n",
      "\n",
      "Epoch 329 | Loss: 9.731354466672098\n",
      "\n",
      "Epoch 330 | Loss: 9.711015325342657\n",
      "\n",
      "Epoch 331 | Loss: 9.690752566860377\n",
      "\n",
      "Epoch 332 | Loss: 9.670565803601702\n",
      "\n",
      "Epoch 333 | Loss: 9.65045464940106\n",
      "\n",
      "Epoch 334 | Loss: 9.630418719560696\n",
      "\n",
      "Epoch 335 | Loss: 9.61045763086008\n",
      "\n",
      "Epoch 336 | Loss: 9.590571001564903\n",
      "\n",
      "Epoch 337 | Loss: 9.570758451435692\n",
      "\n",
      "Epoch 338 | Loss: 9.551019601736014\n",
      "\n",
      "Epoch 339 | Loss: 9.531354075240312\n",
      "\n",
      "Epoch 340 | Loss: 9.511761496241355\n",
      "\n",
      "Epoch 341 | Loss: 9.492241490557332\n",
      "\n",
      "Epoch 342 | Loss: 9.472793685538562\n",
      "\n",
      "Epoch 343 | Loss: 9.453417710073888\n",
      "\n",
      "Epoch 344 | Loss: 9.434113194596677\n",
      "\n",
      "Epoch 345 | Loss: 9.414879771090533\n",
      "\n",
      "Epoch 346 | Loss: 9.395717073094632\n",
      "\n",
      "Epoch 347 | Loss: 9.376624735708761\n",
      "\n",
      "Epoch 348 | Loss: 9.357602395598025\n",
      "\n",
      "Epoch 349 | Loss: 9.338649690997244\n",
      "\n",
      "Epoch 350 | Loss: 9.319766261715051\n",
      "\n",
      "Epoch 351 | Loss: 9.300951749137678\n",
      "\n",
      "Epoch 352 | Loss: 9.282205796232471\n",
      "\n",
      "Epoch 353 | Loss: 9.263528047551084\n",
      "\n",
      "Epoch 354 | Loss: 9.244918149232433\n",
      "\n",
      "Epoch 355 | Loss: 9.226375749005348\n",
      "\n",
      "Epoch 356 | Loss: 9.207900496190963\n",
      "\n",
      "Epoch 357 | Loss: 9.189492041704847\n",
      "\n",
      "Epoch 358 | Loss: 9.171150038058885\n",
      "\n",
      "Epoch 359 | Loss: 9.152874139362883\n",
      "\n",
      "Epoch 360 | Loss: 9.13466400132596\n",
      "\n",
      "Epoch 361 | Loss: 9.116519281257677\n",
      "\n",
      "Epoch 362 | Loss: 9.09843963806894\n",
      "\n",
      "Epoch 363 | Loss: 9.080424732272677\n",
      "\n",
      "Epoch 364 | Loss: 9.062474225984275\n",
      "\n",
      "Epoch 365 | Loss: 9.044587782921825\n",
      "\n",
      "Epoch 366 | Loss: 9.026765068406132\n",
      "\n",
      "Epoch 367 | Loss: 9.009005749360519\n",
      "\n",
      "Epoch 368 | Loss: 8.99130949431043\n",
      "\n",
      "Epoch 369 | Loss: 8.973675973382848\n",
      "\n",
      "Epoch 370 | Loss: 8.956104858305485\n",
      "\n",
      "Epoch 371 | Loss: 8.93859582240582\n",
      "\n",
      "Epoch 372 | Loss: 8.921148540609927\n",
      "\n",
      "Epoch 373 | Loss: 8.903762689441123\n",
      "\n",
      "Epoch 374 | Loss: 8.886437947018466\n",
      "\n",
      "Epoch 375 | Loss: 8.869173993055036\n",
      "\n",
      "Epoch 376 | Loss: 8.851970508856105\n",
      "\n",
      "Epoch 377 | Loss: 8.834827177317079\n",
      "\n",
      "Epoch 378 | Loss: 8.817743682921346\n",
      "\n",
      "Epoch 379 | Loss: 8.80071971173791\n",
      "\n",
      "Epoch 380 | Loss: 8.783754951418931\n",
      "\n",
      "Epoch 381 | Loss: 8.766849091197066\n",
      "\n",
      "Epoch 382 | Loss: 8.750001821882691\n",
      "\n",
      "Epoch 383 | Loss: 8.73321283586099\n",
      "\n",
      "Epoch 384 | Loss: 8.716481827088897\n",
      "\n",
      "Epoch 385 | Loss: 8.699808491091897\n",
      "\n",
      "Epoch 386 | Loss: 8.683192524960736\n",
      "\n",
      "Epoch 387 | Loss: 8.666633627347947\n",
      "\n",
      "Epoch 388 | Loss: 8.650131498464308\n",
      "\n",
      "Epoch 389 | Loss: 8.63368584007515\n",
      "\n",
      "Epoch 390 | Loss: 8.617296355496574\n",
      "\n",
      "Epoch 391 | Loss: 8.60096274959152\n",
      "\n",
      "Epoch 392 | Loss: 8.584684728765778\n",
      "\n",
      "Epoch 393 | Loss: 8.568462000963837\n",
      "\n",
      "Epoch 394 | Loss: 8.552294275664693\n",
      "\n",
      "Epoch 395 | Loss: 8.53618126387749\n",
      "\n",
      "Epoch 396 | Loss: 8.520122678137126\n",
      "\n",
      "Epoch 397 | Loss: 8.504118232499737\n",
      "\n",
      "Epoch 398 | Loss: 8.488167642538082\n",
      "\n",
      "Epoch 399 | Loss: 8.472270625336863\n",
      "\n",
      "Epoch 400 | Loss: 8.456426899487948\n",
      "\n",
      "Epoch 401 | Loss: 8.440636185085502\n",
      "\n",
      "Epoch 402 | Loss: 8.424898203721071\n",
      "\n",
      "Epoch 403 | Loss: 8.409212678478546\n",
      "\n",
      "Epoch 404 | Loss: 8.39357933392909\n",
      "\n",
      "Epoch 405 | Loss: 8.37799789612597\n",
      "\n",
      "Epoch 406 | Loss: 8.362468092599332\n",
      "\n",
      "Epoch 407 | Loss: 8.346989652350903\n",
      "\n",
      "Epoch 408 | Loss: 8.331562305848623\n",
      "\n",
      "Epoch 409 | Loss: 8.316185785021242\n",
      "\n",
      "Epoch 410 | Loss: 8.300859823252818\n",
      "\n",
      "Epoch 411 | Loss: 8.285584155377185\n",
      "\n",
      "Epoch 412 | Loss: 8.270358517672351\n",
      "\n",
      "Epoch 413 | Loss: 8.255182647854852\n",
      "\n",
      "Epoch 414 | Loss: 8.240056285074067\n",
      "\n",
      "Epoch 415 | Loss: 8.224979169906431\n",
      "\n",
      "Epoch 416 | Loss: 8.209951044349681\n",
      "\n",
      "Epoch 417 | Loss: 8.19497165181698\n",
      "\n",
      "Epoch 418 | Loss: 8.180040737131048\n",
      "\n",
      "Epoch 419 | Loss: 8.165158046518231\n",
      "\n",
      "Epoch 420 | Loss: 8.150323327602544\n",
      "\n",
      "Epoch 421 | Loss: 8.135536329399642\n",
      "\n",
      "Epoch 422 | Loss: 8.12079680231081\n",
      "\n",
      "Epoch 423 | Loss: 8.106104498116869\n",
      "\n",
      "Epoch 424 | Loss: 8.091459169972074\n",
      "\n",
      "Epoch 425 | Loss: 8.076860572397978\n",
      "\n",
      "Epoch 426 | Loss: 8.062308461277267\n",
      "\n",
      "Epoch 427 | Loss: 8.047802593847559\n",
      "\n",
      "Epoch 428 | Loss: 8.033342728695185\n",
      "\n",
      "Epoch 429 | Loss: 8.018928625748954\n",
      "\n",
      "Epoch 430 | Loss: 8.00456004627387\n",
      "\n",
      "Epoch 431 | Loss: 7.990236752864847\n",
      "\n",
      "Epoch 432 | Loss: 7.975958509440415\n",
      "\n",
      "Epoch 433 | Loss: 7.961725081236363\n",
      "\n",
      "Epoch 434 | Loss: 7.94753623479941\n",
      "\n",
      "Epoch 435 | Loss: 7.933391737980852\n",
      "\n",
      "Epoch 436 | Loss: 7.919291359930155\n",
      "\n",
      "Epoch 437 | Loss: 7.905234871088606\n",
      "\n",
      "Epoch 438 | Loss: 7.891222043182869\n",
      "\n",
      "Epoch 439 | Loss: 7.877252649218607\n",
      "\n",
      "Epoch 440 | Loss: 7.863326463474031\n",
      "\n",
      "Epoch 441 | Loss: 7.849443261493487\n",
      "\n",
      "Epoch 442 | Loss: 7.835602820081006\n",
      "\n",
      "Epoch 443 | Loss: 7.821804917293868\n",
      "\n",
      "Epoch 444 | Loss: 7.808049332436138\n",
      "\n",
      "Epoch 445 | Loss: 7.794335846052224\n",
      "\n",
      "Epoch 446 | Loss: 7.780664239920407\n",
      "\n",
      "Epoch 447 | Loss: 7.767034297046389\n",
      "\n",
      "Epoch 448 | Loss: 7.753445801656835\n",
      "\n",
      "Epoch 449 | Loss: 7.739898539192889\n",
      "\n",
      "Epoch 450 | Loss: 7.7263922963037395\n",
      "\n",
      "Epoch 451 | Loss: 7.712926860840141\n",
      "\n",
      "Epoch 452 | Loss: 7.699502021847967\n",
      "\n",
      "Epoch 453 | Loss: 7.68611756956175\n",
      "\n",
      "Epoch 454 | Loss: 7.672773295398231\n",
      "\n",
      "Epoch 455 | Loss: 7.659468991949925\n",
      "\n",
      "Epoch 456 | Loss: 7.646204452978666\n",
      "\n",
      "Epoch 457 | Loss: 7.632979473409189\n",
      "\n",
      "Epoch 458 | Loss: 7.619793849322689\n",
      "\n",
      "Epoch 459 | Loss: 7.606647377950424\n",
      "\n",
      "Epoch 460 | Loss: 7.59353985766729\n",
      "\n",
      "Epoch 461 | Loss: 7.580471087985435\n",
      "\n",
      "Epoch 462 | Loss: 7.567440869547848\n",
      "\n",
      "Epoch 463 | Loss: 7.554449004122021\n",
      "\n",
      "Epoch 464 | Loss: 7.5414952945935365\n",
      "\n",
      "Epoch 465 | Loss: 7.528579544959752\n",
      "\n",
      "Epoch 466 | Loss: 7.515701560323433\n",
      "\n",
      "Epoch 467 | Loss: 7.502861146886442\n",
      "\n",
      "Epoch 468 | Loss: 7.490058111943411\n",
      "\n",
      "Epoch 469 | Loss: 7.477292263875459\n",
      "\n",
      "Epoch 470 | Loss: 7.464563412143882\n",
      "\n",
      "Epoch 471 | Loss: 7.451871367283915\n",
      "\n",
      "Epoch 472 | Loss: 7.439215940898456\n",
      "\n",
      "Epoch 473 | Loss: 7.426596945651835\n",
      "\n",
      "Epoch 474 | Loss: 7.414014195263603\n",
      "\n",
      "Epoch 475 | Loss: 7.401467504502322\n",
      "\n",
      "Epoch 476 | Loss: 7.388956689179383\n",
      "\n",
      "Epoch 477 | Loss: 7.3764815661428464\n",
      "\n",
      "Epoch 478 | Loss: 7.364041953271285\n",
      "\n",
      "Epoch 479 | Loss: 7.351637669467669\n",
      "\n",
      "Epoch 480 | Loss: 7.33926853465325\n",
      "\n",
      "Epoch 481 | Loss: 7.326934369761476\n",
      "\n",
      "Epoch 482 | Loss: 7.3146349967319235\n",
      "\n",
      "Epoch 483 | Loss: 7.302370238504254\n",
      "\n",
      "Epoch 484 | Loss: 7.2901399190121925\n",
      "\n",
      "Epoch 485 | Loss: 7.277943863177511\n",
      "\n",
      "Epoch 486 | Loss: 7.265781896904056\n",
      "\n",
      "Epoch 487 | Loss: 7.253653847071795\n",
      "\n",
      "Epoch 488 | Loss: 7.241559541530861\n",
      "\n",
      "Epoch 489 | Loss: 7.229498809095658\n",
      "\n",
      "Epoch 490 | Loss: 7.217471479538954\n",
      "\n",
      "Epoch 491 | Loss: 7.205477383586023\n",
      "\n",
      "Epoch 492 | Loss: 7.193516352908788\n",
      "\n",
      "Epoch 493 | Loss: 7.181588220120019\n",
      "\n",
      "Epoch 494 | Loss: 7.169692818767519\n",
      "\n",
      "Epoch 495 | Loss: 7.157829983328355\n",
      "\n",
      "Epoch 496 | Loss: 7.1459995492031165\n",
      "\n",
      "Epoch 497 | Loss: 7.134201352710179\n",
      "\n",
      "Epoch 498 | Loss: 7.122435231080022\n",
      "\n",
      "Epoch 499 | Loss: 7.11070102244954\n",
      "\n",
      "Epoch 500 | Loss: 7.098998565856404\n",
      "\n",
      "Epoch 501 | Loss: 7.087327701233429\n",
      "\n",
      "Epoch 502 | Loss: 7.075688269402985\n",
      "\n",
      "Epoch 503 | Loss: 7.0640801120714185\n",
      "\n",
      "Epoch 504 | Loss: 7.052503071823519\n",
      "\n",
      "Epoch 505 | Loss: 7.040956992116976\n",
      "\n",
      "Epoch 506 | Loss: 7.029441717276909\n",
      "\n",
      "Epoch 507 | Loss: 7.0179570924903825\n",
      "\n",
      "Epoch 508 | Loss: 7.006502963800981\n",
      "\n",
      "Epoch 509 | Loss: 6.995079178103379\n",
      "\n",
      "Epoch 510 | Loss: 6.983685583137959\n",
      "\n",
      "Epoch 511 | Loss: 6.972322027485458\n",
      "\n",
      "Epoch 512 | Loss: 6.960988360561617\n",
      "\n",
      "Epoch 513 | Loss: 6.949684432611886\n",
      "\n",
      "Epoch 514 | Loss: 6.938410094706148\n",
      "\n",
      "Epoch 515 | Loss: 6.927165198733445\n",
      "\n",
      "Epoch 516 | Loss: 6.915949597396772\n",
      "\n",
      "Epoch 517 | Loss: 6.904763144207871\n",
      "\n",
      "Epoch 518 | Loss: 6.893605693482051\n",
      "\n",
      "Epoch 519 | Loss: 6.8824771003330625\n",
      "\n",
      "Epoch 520 | Loss: 6.8713772206679575\n",
      "\n",
      "Epoch 521 | Loss: 6.860305911182019\n",
      "\n",
      "Epoch 522 | Loss: 6.849263029353688\n",
      "\n",
      "Epoch 523 | Loss: 6.8382484334395315\n",
      "\n",
      "Epoch 524 | Loss: 6.827261982469233\n",
      "\n",
      "Epoch 525 | Loss: 6.816303536240619\n",
      "\n",
      "Epoch 526 | Loss: 6.805372955314698\n",
      "\n",
      "Epoch 527 | Loss: 6.794470101010741\n",
      "\n",
      "Epoch 528 | Loss: 6.783594835401387\n",
      "\n",
      "Epoch 529 | Loss: 6.772747021307764\n",
      "\n",
      "Epoch 530 | Loss: 6.76192652229465\n",
      "\n",
      "Epoch 531 | Loss: 6.751133202665667\n",
      "\n",
      "Epoch 532 | Loss: 6.740366927458483\n",
      "\n",
      "Epoch 533 | Loss: 6.729627562440059\n",
      "\n",
      "Epoch 534 | Loss: 6.718914974101908\n",
      "\n",
      "Epoch 535 | Loss: 6.708229029655403\n",
      "\n",
      "Epoch 536 | Loss: 6.697569597027092\n",
      "\n",
      "Epoch 537 | Loss: 6.686936544854044\n",
      "\n",
      "Epoch 538 | Loss: 6.6763297424792345\n",
      "\n",
      "Epoch 539 | Loss: 6.665749059946953\n",
      "\n",
      "Epoch 540 | Loss: 6.655194367998218\n",
      "\n",
      "Epoch 541 | Loss: 6.644665538066256\n",
      "\n",
      "Epoch 542 | Loss: 6.634162442271978\n",
      "\n",
      "Epoch 543 | Loss: 6.623684953419496\n",
      "\n",
      "Epoch 544 | Loss: 6.613232944991658\n",
      "\n",
      "Epoch 545 | Loss: 6.602806291145627\n",
      "\n",
      "Epoch 546 | Loss: 6.592404866708466\n",
      "\n",
      "Epoch 547 | Loss: 6.582028547172765\n",
      "\n",
      "Epoch 548 | Loss: 6.57167720869229\n",
      "\n",
      "Epoch 549 | Loss: 6.561350728077653\n",
      "\n",
      "Epoch 550 | Loss: 6.551048982792018\n",
      "\n",
      "Epoch 551 | Loss: 6.540771850946834\n",
      "\n",
      "Epoch 552 | Loss: 6.5305192112975865\n",
      "\n",
      "Epoch 553 | Loss: 6.520290943239576\n",
      "\n",
      "Epoch 554 | Loss: 6.510086926803735\n",
      "\n",
      "Epoch 555 | Loss: 6.499907042652455\n",
      "\n",
      "Epoch 556 | Loss: 6.489751172075457\n",
      "\n",
      "Epoch 557 | Loss: 6.479619196985661\n",
      "\n",
      "Epoch 558 | Loss: 6.469510999915131\n",
      "\n",
      "Epoch 559 | Loss: 6.459426464010978\n",
      "\n",
      "Epoch 560 | Loss: 6.449365473031353\n",
      "\n",
      "Epoch 561 | Loss: 6.439327911341427\n",
      "\n",
      "Epoch 562 | Loss: 6.429313663909412\n",
      "\n",
      "Epoch 563 | Loss: 6.419322616302602\n",
      "\n",
      "Epoch 564 | Loss: 6.409354654683441\n",
      "\n",
      "Epoch 565 | Loss: 6.399409665805618\n",
      "\n",
      "Epoch 566 | Loss: 6.38948753701019\n",
      "\n",
      "Epoch 567 | Loss: 6.3795881562217245\n",
      "\n",
      "Epoch 568 | Loss: 6.369711411944466\n",
      "\n",
      "Epoch 569 | Loss: 6.359857193258537\n",
      "\n",
      "Epoch 570 | Loss: 6.350025389816156\n",
      "\n",
      "Epoch 571 | Loss: 6.340215891837883\n",
      "\n",
      "Epoch 572 | Loss: 6.330428590108891\n",
      "\n",
      "Epoch 573 | Loss: 6.320663375975254\n",
      "\n",
      "Epoch 574 | Loss: 6.3109201413402785\n",
      "\n",
      "Epoch 575 | Loss: 6.301198778660836\n",
      "\n",
      "Epoch 576 | Loss: 6.29149918094374\n",
      "\n",
      "Epoch 577 | Loss: 6.281821241742131\n",
      "\n",
      "Epoch 578 | Loss: 6.272164855151897\n",
      "\n",
      "Epoch 579 | Loss: 6.262529915808117\n",
      "\n",
      "Epoch 580 | Loss: 6.252916318881525\n",
      "\n",
      "Epoch 581 | Loss: 6.243323960074988\n",
      "\n",
      "Epoch 582 | Loss: 6.2337527356200395\n",
      "\n",
      "Epoch 583 | Loss: 6.224202542273397\n",
      "\n",
      "Epoch 584 | Loss: 6.214673277313527\n",
      "\n",
      "Epoch 585 | Loss: 6.205164838537236\n",
      "\n",
      "Epoch 586 | Loss: 6.195677124256263\n",
      "\n",
      "Epoch 587 | Loss: 6.1862100332939205\n",
      "\n",
      "Epoch 588 | Loss: 6.176763464981742\n",
      "\n",
      "Epoch 589 | Loss: 6.167337319156157\n",
      "\n",
      "Epoch 590 | Loss: 6.1579314961551965\n",
      "\n",
      "Epoch 591 | Loss: 6.1485458968151985\n",
      "\n",
      "Epoch 592 | Loss: 6.139180422467574\n",
      "\n",
      "Epoch 593 | Loss: 6.129834974935552\n",
      "\n",
      "Epoch 594 | Loss: 6.120509456530982\n",
      "\n",
      "Epoch 595 | Loss: 6.111203770051133\n",
      "\n",
      "Epoch 596 | Loss: 6.101917818775547\n",
      "\n",
      "Epoch 597 | Loss: 6.092651506462869\n",
      "\n",
      "Epoch 598 | Loss: 6.08340473734774\n",
      "\n",
      "Epoch 599 | Loss: 6.07417741613769\n",
      "\n",
      "Epoch 600 | Loss: 6.064969448010069\n",
      "\n",
      "Epoch 601 | Loss: 6.055780738608963\n",
      "\n",
      "Epoch 602 | Loss: 6.046611194042193\n",
      "\n",
      "Epoch 603 | Loss: 6.037460720878272\n",
      "\n",
      "Epoch 604 | Loss: 6.028329226143419\n",
      "\n",
      "Epoch 605 | Loss: 6.019216617318594\n",
      "\n",
      "Epoch 606 | Loss: 6.010122802336537\n",
      "\n",
      "Epoch 607 | Loss: 6.001047689578835\n",
      "\n",
      "Epoch 608 | Loss: 5.991991187873031\n",
      "\n",
      "Epoch 609 | Loss: 5.982953206489705\n",
      "\n",
      "Epoch 610 | Loss: 5.9739336551396285\n",
      "\n",
      "Epoch 611 | Loss: 5.9649324439709055\n",
      "\n",
      "Epoch 612 | Loss: 5.955949483566145\n",
      "\n",
      "Epoch 613 | Loss: 5.946984684939655\n",
      "\n",
      "Epoch 614 | Loss: 5.938037959534654\n",
      "\n",
      "Epoch 615 | Loss: 5.929109219220501\n",
      "\n",
      "Epoch 616 | Loss: 5.920198376289945\n",
      "\n",
      "Epoch 617 | Loss: 5.911305343456405\n",
      "\n",
      "Epoch 618 | Loss: 5.902430033851253\n",
      "\n",
      "Epoch 619 | Loss: 5.893572361021127\n",
      "\n",
      "Epoch 620 | Loss: 5.884732238925258\n",
      "\n",
      "Epoch 621 | Loss: 5.875909581932823\n",
      "\n",
      "Epoch 622 | Loss: 5.867104304820315\n",
      "\n",
      "Epoch 623 | Loss: 5.858316322768917\n",
      "\n",
      "Epoch 624 | Loss: 5.849545551361931\n",
      "\n",
      "Epoch 625 | Loss: 5.84079190658218\n",
      "\n",
      "Epoch 626 | Loss: 5.832055304809464\n",
      "\n",
      "Epoch 627 | Loss: 5.823335662818021\n",
      "\n",
      "Epoch 628 | Loss: 5.814632897774\n",
      "\n",
      "Epoch 629 | Loss: 5.805946927232974\n",
      "\n",
      "Epoch 630 | Loss: 5.797277669137446\n",
      "\n",
      "Epoch 631 | Loss: 5.7886250418143845\n",
      "\n",
      "Epoch 632 | Loss: 5.779988963972781\n",
      "\n",
      "Epoch 633 | Loss: 5.771369354701224\n",
      "\n",
      "Epoch 634 | Loss: 5.762766133465479\n",
      "\n",
      "Epoch 635 | Loss: 5.754179220106101\n",
      "\n",
      "Epoch 636 | Loss: 5.745608534836061\n",
      "\n",
      "Epoch 637 | Loss: 5.737053998238384\n",
      "\n",
      "Epoch 638 | Loss: 5.728515531263806\n",
      "\n",
      "Epoch 639 | Loss: 5.71999305522846\n",
      "\n",
      "Epoch 640 | Loss: 5.711486491811553\n",
      "\n",
      "Epoch 641 | Loss: 5.7029957630530985\n",
      "\n",
      "Epoch 642 | Loss: 5.694520791351625\n",
      "\n",
      "Epoch 643 | Loss: 5.686061499461934\n",
      "\n",
      "Epoch 644 | Loss: 5.677617810492848\n",
      "\n",
      "Epoch 645 | Loss: 5.669189647905\n",
      "\n",
      "Epoch 646 | Loss: 5.660776935508621\n",
      "\n",
      "Epoch 647 | Loss: 5.652379597461359\n",
      "\n",
      "Epoch 648 | Loss: 5.643997558266092\n",
      "\n",
      "Epoch 649 | Loss: 5.635630742768785\n",
      "\n",
      "Epoch 650 | Loss: 5.627279076156344\n",
      "\n",
      "Epoch 651 | Loss: 5.618942483954488\n",
      "\n",
      "Epoch 652 | Loss: 5.610620892025654\n",
      "\n",
      "Epoch 653 | Loss: 5.602314226566881\n",
      "\n",
      "Epoch 654 | Loss: 5.594022414107754\n",
      "\n",
      "Epoch 655 | Loss: 5.585745381508335\n",
      "\n",
      "Epoch 656 | Loss: 5.577483055957118\n",
      "\n",
      "Epoch 657 | Loss: 5.569235364968994\n",
      "\n",
      "Epoch 658 | Loss: 5.561002236383238\n",
      "\n",
      "Epoch 659 | Loss: 5.552783598361523\n",
      "\n",
      "Epoch 660 | Loss: 5.544579379385907\n",
      "\n",
      "Epoch 661 | Loss: 5.536389508256889\n",
      "\n",
      "Epoch 662 | Loss: 5.528213914091448\n",
      "\n",
      "Epoch 663 | Loss: 5.520052526321091\n",
      "\n",
      "Epoch 664 | Loss: 5.511905274689944\n",
      "\n",
      "Epoch 665 | Loss: 5.503772089252831\n",
      "\n",
      "Epoch 666 | Loss: 5.495652900373379\n",
      "\n",
      "Epoch 667 | Loss: 5.48754763872215\n",
      "\n",
      "Epoch 668 | Loss: 5.479456235274754\n",
      "\n",
      "Epoch 669 | Loss: 5.471378621310006\n",
      "\n",
      "Epoch 670 | Loss: 5.463314728408099\n",
      "\n",
      "Epoch 671 | Loss: 5.455264488448751\n",
      "\n",
      "Epoch 672 | Loss: 5.447227833609431\n",
      "\n",
      "Epoch 673 | Loss: 5.439204696363529\n",
      "\n",
      "Epoch 674 | Loss: 5.431195009478599\n",
      "\n",
      "Epoch 675 | Loss: 5.423198706014573\n",
      "\n",
      "Epoch 676 | Loss: 5.415215719322017\n",
      "\n",
      "Epoch 677 | Loss: 5.407245983040388\n",
      "\n",
      "Epoch 678 | Loss: 5.399289431096301\n",
      "\n",
      "Epoch 679 | Loss: 5.39134599770182\n",
      "\n",
      "Epoch 680 | Loss: 5.383415617352755\n",
      "\n",
      "Epoch 681 | Loss: 5.375498224826974\n",
      "\n",
      "Epoch 682 | Loss: 5.36759375518273\n",
      "\n",
      "Epoch 683 | Loss: 5.359702143757\n",
      "\n",
      "Epoch 684 | Loss: 5.351823326163827\n",
      "\n",
      "Epoch 685 | Loss: 5.343957238292706\n",
      "\n",
      "Epoch 686 | Loss: 5.336103816306944\n",
      "\n",
      "Epoch 687 | Loss: 5.328262996642049\n",
      "\n",
      "Epoch 688 | Loss: 5.320434716004147\n",
      "\n",
      "Epoch 689 | Loss: 5.312618911368387\n",
      "\n",
      "Epoch 690 | Loss: 5.30481551997737\n",
      "\n",
      "Epoch 691 | Loss: 5.297024479339591\n",
      "\n",
      "Epoch 692 | Loss: 5.289245727227894\n",
      "\n",
      "Epoch 693 | Loss: 5.281479201677938\n",
      "\n",
      "Epoch 694 | Loss: 5.273724840986671\n",
      "\n",
      "Epoch 695 | Loss: 5.265982583710818\n",
      "\n",
      "Epoch 696 | Loss: 5.25825236866539\n",
      "\n",
      "Epoch 697 | Loss: 5.250534134922195\n",
      "\n",
      "Epoch 698 | Loss: 5.242827821808362\n",
      "\n",
      "Epoch 699 | Loss: 5.235133368904876\n",
      "\n",
      "Epoch 700 | Loss: 5.227450716045137\n",
      "\n",
      "Epoch 701 | Loss: 5.219779803313511\n",
      "\n",
      "Epoch 702 | Loss: 5.21212057104391\n",
      "\n",
      "Epoch 703 | Loss: 5.204472959818376\n",
      "\n",
      "Epoch 704 | Loss: 5.1968369104656675\n",
      "\n",
      "Epoch 705 | Loss: 5.189212364059885\n",
      "\n",
      "Epoch 706 | Loss: 5.181599261919071\n",
      "\n",
      "Epoch 707 | Loss: 5.173997545603852\n",
      "\n",
      "Epoch 708 | Loss: 5.166407156916077\n",
      "\n",
      "Epoch 709 | Loss: 5.15882803789747\n",
      "\n",
      "Epoch 710 | Loss: 5.151260130828288\n",
      "\n",
      "Epoch 711 | Loss: 5.143703378226013\n",
      "\n",
      "Epoch 712 | Loss: 5.136157722844014\n",
      "\n",
      "Epoch 713 | Loss: 5.1286231076702595\n",
      "\n",
      "Epoch 714 | Loss: 5.121099475926023\n",
      "\n",
      "Epoch 715 | Loss: 5.113586771064596\n",
      "\n",
      "Epoch 716 | Loss: 5.106084936770023\n",
      "\n",
      "Epoch 717 | Loss: 5.098593916955837\n",
      "\n",
      "Epoch 718 | Loss: 5.0911136557638095\n",
      "\n",
      "Epoch 719 | Loss: 5.0836440975627175\n",
      "\n",
      "Epoch 720 | Loss: 5.0761851869471055\n",
      "\n",
      "Epoch 721 | Loss: 5.068736868736071\n",
      "\n",
      "Epoch 722 | Loss: 5.061299087972064\n",
      "\n",
      "Epoch 723 | Loss: 5.053871789919677\n",
      "\n",
      "Epoch 724 | Loss: 5.046454920064462\n",
      "\n",
      "Epoch 725 | Loss: 5.039048424111752\n",
      "\n",
      "Epoch 726 | Loss: 5.031652247985504\n",
      "\n",
      "Epoch 727 | Loss: 5.024266337827128\n",
      "\n",
      "Epoch 728 | Loss: 5.016890639994345\n",
      "\n",
      "Epoch 729 | Loss: 5.009525101060049\n",
      "\n",
      "Epoch 730 | Loss: 5.002169667811184\n",
      "\n",
      "Epoch 731 | Loss: 4.994824287247622\n",
      "\n",
      "Epoch 732 | Loss: 4.987488906581051\n",
      "\n",
      "Epoch 733 | Loss: 4.98016347323389\n",
      "\n",
      "Epoch 734 | Loss: 4.972847934838187\n",
      "\n",
      "Epoch 735 | Loss: 4.965542239234547\n",
      "\n",
      "Epoch 736 | Loss: 4.958246334471059\n",
      "\n",
      "Epoch 737 | Loss: 4.950960168802239\n",
      "\n",
      "Epoch 738 | Loss: 4.943683690687983\n",
      "\n",
      "Epoch 739 | Loss: 4.936416848792515\n",
      "\n",
      "Epoch 740 | Loss: 4.929159591983363\n",
      "\n",
      "Epoch 741 | Loss: 4.921911869330341\n",
      "\n",
      "Epoch 742 | Loss: 4.914673630104519\n",
      "\n",
      "Epoch 743 | Loss: 4.907444823777244\n",
      "\n",
      "Epoch 744 | Loss: 4.900225400019119\n",
      "\n",
      "Epoch 745 | Loss: 4.893015308699035\n",
      "\n",
      "Epoch 746 | Loss: 4.885814499883185\n",
      "\n",
      "Epoch 747 | Loss: 4.8786229238341\n",
      "\n",
      "Epoch 748 | Loss: 4.87144053100969\n",
      "\n",
      "Epoch 749 | Loss: 4.864267272062293\n",
      "\n",
      "Epoch 750 | Loss: 4.85710309783773\n",
      "\n",
      "Epoch 751 | Loss: 4.849947959374382\n",
      "\n",
      "Epoch 752 | Loss: 4.842801807902251\n",
      "\n",
      "Epoch 753 | Loss: 4.835664594842065\n",
      "\n",
      "Epoch 754 | Loss: 4.8285362718043485\n",
      "\n",
      "Epoch 755 | Loss: 4.821416790588542\n",
      "\n",
      "Epoch 756 | Loss: 4.81430610318211\n",
      "\n",
      "Epoch 757 | Loss: 4.807204161759653\n",
      "\n",
      "Epoch 758 | Loss: 4.800110918682035\n",
      "\n",
      "Epoch 759 | Loss: 4.79302632649553\n",
      "\n",
      "Epoch 760 | Loss: 4.785950337930953\n",
      "\n",
      "Epoch 761 | Loss: 4.778882905902825\n",
      "\n",
      "Epoch 762 | Loss: 4.77182398350852\n",
      "\n",
      "Epoch 763 | Loss: 4.76477352402745\n",
      "\n",
      "Epoch 764 | Loss: 4.757731480920223\n",
      "\n",
      "Epoch 765 | Loss: 4.750697807827851\n",
      "\n",
      "Epoch 766 | Loss: 4.743672458570918\n",
      "\n",
      "Epoch 767 | Loss: 4.736655387148806\n",
      "\n",
      "Epoch 768 | Loss: 4.729646547738887\n",
      "\n",
      "Epoch 769 | Loss: 4.722645894695746\n",
      "\n",
      "Epoch 770 | Loss: 4.715653382550407\n",
      "\n",
      "Epoch 771 | Loss: 4.708668966009568\n",
      "\n",
      "Epoch 772 | Loss: 4.701692599954837\n",
      "\n",
      "Epoch 773 | Loss: 4.694724239441985\n",
      "\n",
      "Epoch 774 | Loss: 4.687763839700198\n",
      "\n",
      "Epoch 775 | Loss: 4.680811356131344\n",
      "\n",
      "Epoch 776 | Loss: 4.673866744309249\n",
      "\n",
      "Epoch 777 | Loss: 4.666929959978971\n",
      "\n",
      "Epoch 778 | Loss: 4.660000959056082\n",
      "\n",
      "Epoch 779 | Loss: 4.6530796976259765\n",
      "\n",
      "Epoch 780 | Loss: 4.646166131943162\n",
      "\n",
      "Epoch 781 | Loss: 4.639260218430575\n",
      "\n",
      "Epoch 782 | Loss: 4.632361913678895\n",
      "\n",
      "Epoch 783 | Loss: 4.625471174445866\n",
      "\n",
      "Epoch 784 | Loss: 4.618587957655636\n",
      "\n",
      "Epoch 785 | Loss: 4.6117122203980925\n",
      "\n",
      "Epoch 786 | Loss: 4.604843919928207\n",
      "\n",
      "Epoch 787 | Loss: 4.5979830136653845\n",
      "\n",
      "Epoch 788 | Loss: 4.59112945919284\n",
      "\n",
      "Epoch 789 | Loss: 4.584283214256947\n",
      "\n",
      "Epoch 790 | Loss: 4.5774442367666275\n",
      "\n",
      "Epoch 791 | Loss: 4.570612484792724\n",
      "\n",
      "Epoch 792 | Loss: 4.563787916567398\n",
      "\n",
      "Epoch 793 | Loss: 4.556970490483517\n",
      "\n",
      "Epoch 794 | Loss: 4.550160165094065\n",
      "\n",
      "Epoch 795 | Loss: 4.543356899111547\n",
      "\n",
      "Epoch 796 | Loss: 4.536560651407415\n",
      "\n",
      "Epoch 797 | Loss: 4.529771381011479\n",
      "\n",
      "Epoch 798 | Loss: 4.5229890471113485\n",
      "\n",
      "Epoch 799 | Loss: 4.516213609051865\n",
      "\n",
      "Epoch 800 | Loss: 4.509445026334554\n",
      "\n",
      "Epoch 801 | Loss: 4.50268325861706\n",
      "\n",
      "Epoch 802 | Loss: 4.495928265712622\n",
      "\n",
      "Epoch 803 | Loss: 4.489180007589527\n",
      "\n",
      "Epoch 804 | Loss: 4.482438444370586\n",
      "\n",
      "Epoch 805 | Loss: 4.475703536332611\n",
      "\n",
      "Epoch 806 | Loss: 4.4689752439058985\n",
      "\n",
      "Epoch 807 | Loss: 4.46225352767372\n",
      "\n",
      "Epoch 808 | Loss: 4.4555383483718245\n",
      "\n",
      "Epoch 809 | Loss: 4.448829666887929\n",
      "\n",
      "Epoch 810 | Loss: 4.4421274442612475\n",
      "\n",
      "Epoch 811 | Loss: 4.435431641681992\n",
      "\n",
      "Epoch 812 | Loss: 4.428742220490904\n",
      "\n",
      "Epoch 813 | Loss: 4.422059142178781\n",
      "\n",
      "Epoch 814 | Loss: 4.415382368386015\n",
      "\n",
      "Epoch 815 | Loss: 4.4087118609021285\n",
      "\n",
      "Epoch 816 | Loss: 4.402047581665331\n",
      "\n",
      "Epoch 817 | Loss: 4.395389492762065\n",
      "\n",
      "Epoch 818 | Loss: 4.388737556426574\n",
      "\n",
      "Epoch 819 | Loss: 4.382091735040465\n",
      "\n",
      "Epoch 820 | Loss: 4.375451991132283\n",
      "\n",
      "Epoch 821 | Loss: 4.36881828737709\n",
      "\n",
      "Epoch 822 | Loss: 4.3621905865960455\n",
      "\n",
      "Epoch 823 | Loss: 4.355568851756009\n",
      "\n",
      "Epoch 824 | Loss: 4.348953045969123\n",
      "\n",
      "Epoch 825 | Loss: 4.342343132492426\n",
      "\n",
      "Epoch 826 | Loss: 4.335739074727463\n",
      "\n",
      "Epoch 827 | Loss: 4.329140836219884\n",
      "\n",
      "Epoch 828 | Loss: 4.3225483806590885\n",
      "\n",
      "Epoch 829 | Loss: 4.315961671877835\n",
      "\n",
      "Epoch 830 | Loss: 4.30938067385188\n",
      "\n",
      "Epoch 831 | Loss: 4.302805350699612\n",
      "\n",
      "Epoch 832 | Loss: 4.296235666681702\n",
      "\n",
      "Epoch 833 | Loss: 4.2896715862007495\n",
      "\n",
      "Epoch 834 | Loss: 4.283113073800933\n",
      "\n",
      "Epoch 835 | Loss: 4.276560094167681\n",
      "\n",
      "Epoch 836 | Loss: 4.270012612127329\n",
      "\n",
      "Epoch 837 | Loss: 4.2634705926468035\n",
      "\n",
      "Epoch 838 | Loss: 4.256934000833288\n",
      "\n",
      "Epoch 839 | Loss: 4.250402801933909\n",
      "\n",
      "Epoch 840 | Loss: 4.243876961335432\n",
      "\n",
      "Epoch 841 | Loss: 4.237356444563948\n",
      "\n",
      "Epoch 842 | Loss: 4.230841217284579\n",
      "\n",
      "Epoch 843 | Loss: 4.224331245301173\n",
      "\n",
      "Epoch 844 | Loss: 4.217826494556035\n",
      "\n",
      "Epoch 845 | Loss: 4.211326931129619\n",
      "\n",
      "Epoch 846 | Loss: 4.204832521240266\n",
      "\n",
      "Epoch 847 | Loss: 4.198343231243922\n",
      "\n",
      "Epoch 848 | Loss: 4.191859027633879\n",
      "\n",
      "Epoch 849 | Loss: 4.185379877040504\n",
      "\n",
      "Epoch 850 | Loss: 4.17890574623098\n",
      "\n",
      "Epoch 851 | Loss: 4.172436602109067\n",
      "\n",
      "Epoch 852 | Loss: 4.165972411714834\n",
      "\n",
      "Epoch 853 | Loss: 4.15951314222444\n",
      "\n",
      "Epoch 854 | Loss: 4.153058760949877\n",
      "\n",
      "Epoch 855 | Loss: 4.146609235338754\n",
      "\n",
      "Epoch 856 | Loss: 4.140164532974057\n",
      "\n",
      "Epoch 857 | Loss: 4.133724621573936\n",
      "\n",
      "Epoch 858 | Loss: 4.127289468991489\n",
      "\n",
      "Epoch 859 | Loss: 4.12085904321454\n",
      "\n",
      "Epoch 860 | Loss: 4.1144333123654455\n",
      "\n",
      "Epoch 861 | Loss: 4.108012244700881\n",
      "\n",
      "Epoch 862 | Loss: 4.1015958086116555\n",
      "\n",
      "Epoch 863 | Loss: 4.095183972622506\n",
      "\n",
      "Epoch 864 | Loss: 4.088776705391922\n",
      "\n",
      "Epoch 865 | Loss: 4.082373975711953\n",
      "\n",
      "Epoch 866 | Loss: 4.075975752508044\n",
      "\n",
      "Epoch 867 | Loss: 4.069582004838849\n",
      "\n",
      "Epoch 868 | Loss: 4.063192701896066\n",
      "\n",
      "Epoch 869 | Loss: 4.056807813004277\n",
      "\n",
      "Epoch 870 | Loss: 4.050427307620793\n",
      "\n",
      "Epoch 871 | Loss: 4.044051155335485\n",
      "\n",
      "Epoch 872 | Loss: 4.037679325870651\n",
      "\n",
      "Epoch 873 | Loss: 4.031311789080859\n",
      "\n",
      "Epoch 874 | Loss: 4.024948514952811\n",
      "\n",
      "Epoch 875 | Loss: 4.018589473605208\n",
      "\n",
      "Epoch 876 | Loss: 4.012234635288618\n",
      "\n",
      "Epoch 877 | Loss: 4.005883970385343\n",
      "\n",
      "Epoch 878 | Loss: 3.999537449409307\n",
      "\n",
      "Epoch 879 | Loss: 3.99319504300593\n",
      "\n",
      "Epoch 880 | Loss: 3.986856721952013\n",
      "\n",
      "Epoch 881 | Loss: 3.980522457155634\n",
      "\n",
      "Epoch 882 | Loss: 3.974192219656043\n",
      "\n",
      "Epoch 883 | Loss: 3.9678659806235546\n",
      "\n",
      "Epoch 884 | Loss: 3.9615437113594543\n",
      "\n",
      "Epoch 885 | Loss: 3.9552253832959123\n",
      "\n",
      "Epoch 886 | Loss: 3.9489109679958823\n",
      "\n",
      "Epoch 887 | Loss: 3.9426004371530325\n",
      "\n",
      "Epoch 888 | Loss: 3.936293762591654\n",
      "\n",
      "Epoch 889 | Loss: 3.9299909162665894\n",
      "\n",
      "Epoch 890 | Loss: 3.923691870263163\n",
      "\n",
      "Epoch 891 | Loss: 3.91739659679711\n",
      "\n",
      "Epoch 892 | Loss: 3.911105068214516\n",
      "\n",
      "Epoch 893 | Loss: 3.9048172569917527\n",
      "\n",
      "Epoch 894 | Loss: 3.89853313573543\n",
      "\n",
      "Epoch 895 | Loss: 3.8922526771823374\n",
      "\n",
      "Epoch 896 | Loss: 3.8859758541993976\n",
      "\n",
      "Epoch 897 | Loss: 3.879702639783628\n",
      "\n",
      "Epoch 898 | Loss: 3.8734330070620935\n",
      "\n",
      "Epoch 899 | Loss: 3.867166929291874\n",
      "\n",
      "Epoch 900 | Loss: 3.8609043798600347\n",
      "\n",
      "Epoch 901 | Loss: 3.854645332283594\n",
      "\n",
      "Epoch 902 | Loss: 3.8483897602094976\n",
      "\n",
      "Epoch 903 | Loss: 3.8421376374146035\n",
      "\n",
      "Epoch 904 | Loss: 3.8358889378056604\n",
      "\n",
      "Epoch 905 | Loss: 3.829643635419292\n",
      "\n",
      "Epoch 906 | Loss: 3.8234017044219946\n",
      "\n",
      "Epoch 907 | Loss: 3.8171631191101234\n",
      "\n",
      "Epoch 908 | Loss: 3.8109278539098934\n",
      "\n",
      "Epoch 909 | Loss: 3.8046958833773785\n",
      "\n",
      "Epoch 910 | Loss: 3.7984671821985194\n",
      "\n",
      "Epoch 911 | Loss: 3.7922417251891267\n",
      "\n",
      "Epoch 912 | Loss: 3.7860194872948965\n",
      "\n",
      "Epoch 913 | Loss: 3.779800443591419\n",
      "\n",
      "Epoch 914 | Loss: 3.7735845692842083\n",
      "\n",
      "Epoch 915 | Loss: 3.767371839708709\n",
      "\n",
      "Epoch 916 | Loss: 3.761162230330336\n",
      "\n",
      "Epoch 917 | Loss: 3.7549557167444862\n",
      "\n",
      "Epoch 918 | Loss: 3.748752274676591\n",
      "\n",
      "Epoch 919 | Loss: 3.7425518799821322\n",
      "\n",
      "Epoch 920 | Loss: 3.73635450864669\n",
      "\n",
      "Epoch 921 | Loss: 3.7301601367859805\n",
      "\n",
      "Epoch 922 | Loss: 3.7239687406459008\n",
      "\n",
      "Epoch 923 | Loss: 3.71778029660258\n",
      "\n",
      "Epoch 924 | Loss: 3.7115947811624266\n",
      "\n",
      "Epoch 925 | Loss: 3.7054121709621812\n",
      "\n",
      "Epoch 926 | Loss: 3.69923244276898\n",
      "\n",
      "Epoch 927 | Loss: 3.693055573480403\n",
      "\n",
      "Epoch 928 | Loss: 3.686881540124549\n",
      "\n",
      "Epoch 929 | Loss: 3.6807103198600934\n",
      "\n",
      "Epoch 930 | Loss: 3.674541889976366\n",
      "\n",
      "Epoch 931 | Loss: 3.6683762278934102\n",
      "\n",
      "Epoch 932 | Loss: 3.6622133111620663\n",
      "\n",
      "Epoch 933 | Loss: 3.656053117464049\n",
      "\n",
      "Epoch 934 | Loss: 3.649895624612027\n",
      "\n",
      "Epoch 935 | Loss: 3.643740810549699\n",
      "\n",
      "Epoch 936 | Loss: 3.637588653351892\n",
      "\n",
      "Epoch 937 | Loss: 3.6314391312246412\n",
      "\n",
      "Epoch 938 | Loss: 3.6252922225052826\n",
      "\n",
      "Epoch 939 | Loss: 3.6191479056625506\n",
      "\n",
      "Epoch 940 | Loss: 3.6130061592966722\n",
      "\n",
      "Epoch 941 | Loss: 3.606866962139463\n",
      "\n",
      "Epoch 942 | Loss: 3.600730293054436\n",
      "\n",
      "Epoch 943 | Loss: 3.594596131036901\n",
      "\n",
      "Epoch 944 | Loss: 3.5884644552140723\n",
      "\n",
      "Epoch 945 | Loss: 3.582335244845184\n",
      "\n",
      "Epoch 946 | Loss: 3.5762084793215894\n",
      "\n",
      "Epoch 947 | Loss: 3.570084138166888\n",
      "\n",
      "Epoch 948 | Loss: 3.5639622010370346\n",
      "\n",
      "Epoch 949 | Loss: 3.5578426477204634\n",
      "\n",
      "Epoch 950 | Loss: 3.5517254581382036\n",
      "\n",
      "Epoch 951 | Loss: 3.5456106123440088\n",
      "\n",
      "Epoch 952 | Loss: 3.5394980905244835\n",
      "\n",
      "Epoch 953 | Loss: 3.5333878729992017\n",
      "\n",
      "Epoch 954 | Loss: 3.5272799402208523\n",
      "\n",
      "Epoch 955 | Loss: 3.521174272775357\n",
      "\n",
      "Epoch 956 | Loss: 3.5150708513820206\n",
      "\n",
      "Epoch 957 | Loss: 3.5089696568936524\n",
      "\n",
      "Epoch 958 | Loss: 3.502870670296714\n",
      "\n",
      "Epoch 959 | Loss: 3.4967738727114623\n",
      "\n",
      "Epoch 960 | Loss: 3.490679245392089\n",
      "\n",
      "Epoch 961 | Loss: 3.4845867697268615\n",
      "\n",
      "Epoch 962 | Loss: 3.4784964272382815\n",
      "\n",
      "Epoch 963 | Loss: 3.4724081995832226\n",
      "\n",
      "Epoch 964 | Loss: 3.4663220685530916\n",
      "\n",
      "Epoch 965 | Loss: 3.4602380160739705\n",
      "\n",
      "Epoch 966 | Loss: 3.454156024206783\n",
      "\n",
      "Epoch 967 | Loss: 3.448076075147439\n",
      "\n",
      "Epoch 968 | Loss: 3.4419981512270006\n",
      "\n",
      "Epoch 969 | Loss: 3.4359222349118452\n",
      "\n",
      "Epoch 970 | Loss: 3.42984830880382\n",
      "\n",
      "Epoch 971 | Loss: 3.423776355640409\n",
      "\n",
      "Epoch 972 | Loss: 3.417706358294896\n",
      "\n",
      "Epoch 973 | Loss: 3.4116382997765338\n",
      "\n",
      "Epoch 974 | Loss: 3.4055721632307154\n",
      "\n",
      "Epoch 975 | Loss: 3.3995079319391364\n",
      "\n",
      "Epoch 976 | Loss: 3.3934455893199744\n",
      "\n",
      "Epoch 977 | Loss: 3.3873851189280555\n",
      "\n",
      "Epoch 978 | Loss: 3.381326504455034\n",
      "\n",
      "Epoch 979 | Loss: 3.375269729729567\n",
      "\n",
      "Epoch 980 | Loss: 3.3692147787174864\n",
      "\n",
      "Epoch 981 | Loss: 3.363161635521991\n",
      "\n",
      "Epoch 982 | Loss: 3.35711028438381\n",
      "\n",
      "Epoch 983 | Loss: 3.3510607096813967\n",
      "\n",
      "Epoch 984 | Loss: 3.345012895931104\n",
      "\n",
      "Epoch 985 | Loss: 3.338966827787375\n",
      "\n",
      "Epoch 986 | Loss: 3.3329224900429226\n",
      "\n",
      "Epoch 987 | Loss: 3.3268798676289197\n",
      "\n",
      "Epoch 988 | Loss: 3.3208389456151814\n",
      "\n",
      "Epoch 989 | Loss: 3.3147997092103627\n",
      "\n",
      "Epoch 990 | Loss: 3.308762143762138\n",
      "\n",
      "Epoch 991 | Loss: 3.302726234757399\n",
      "\n",
      "Epoch 992 | Loss: 3.296691967822444\n",
      "\n",
      "Epoch 993 | Loss: 3.2906593287231733\n",
      "\n",
      "Epoch 994 | Loss: 3.284628303365277\n",
      "\n",
      "Epoch 995 | Loss: 3.278598877794437\n",
      "\n",
      "Epoch 996 | Loss: 3.2725710381965194\n",
      "\n",
      "Epoch 997 | Loss: 3.26654477089777\n",
      "\n",
      "Epoch 998 | Loss: 3.260520062365014\n",
      "\n",
      "Epoch 999 | Loss: 3.254496899205851\n"
     ]
    }
   ],
   "source": [
    "model = Model(layer=Dense)\n",
    "model.compile(loss=MSELoss(), optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "X_train = np.random.normal(0, 1, (100,3))\n",
    "y_train = np.array(\n",
    "    [[x1 * 1.5 + x3 * 3.5, x2 * 2.5 + x3 * 3.5] for x1, x2, x3 in X_train]\n",
    ")\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
